//////////////////////////函数说明//////////////////////////

pts: AV_NOPTS_VALUE
dts: AV_NOPTS_VALUE
pos: 0

int av_parser_parse2(AVCodecParserContext *s,
                     AVCodecContext *avctx,
                     uint8_t **poutbuf, int *poutbuf_size,
                     const uint8_t *buf, int buf_size,
                     int64_t pts, int64_t dts,
                     int64_t pos);

使用AVCodecParser从输入的数据流中分离出一帧一帧的压缩编码数据。
AVCodecParser用于解析输入的数据流并把它们分成一帧一帧的压缩编码数据。
比较形象的说法就是把长长的一段连续的数据“切割”成一段段的数据。
核心函数是av_parser_parse2().
解析数据获得一个Packet， 从输入的数据流中分离出一帧一帧的压缩编码数据。
通过av_parser_parse2拿到AVPaket数据，跟av_read_frame类似.
输入必须是只包含视频编码数据“裸流”（例如H.264、HEVC码流文件），
而不能是包含封装格式的媒体数据（例如AVI、MKV、MP4）.

AVCodec *avcodec_find_decoder(enum AVCodecID id);
// 通过code ID查找一个已经注册的音视频解码器
// 引入 #include "libavcodec/avcodec.h"
// 实现在: \ffmpeg\libavcodec\utils.c
// 查找成功返回解码器指针,否则返回NULL
// 音视频解码器保存在一个链表中,查找过程中,函数从头到尾遍历链表,通过比较解码器的ID来查找

AVCodec *avcodec_find_decoder_by_name(const char *name);
// 通过一个指定的名称查找一个已经注册的音视频解码器
// 引入 #include "libavcodec/avcodec.h"
// 实现在: \ffmpeg\libavcodec\utils.c
// 查找成功返回解码器指针,否则返回NULL
// 音视频解码器保存在一个链表中,查找过程中,函数从头到尾遍历链表,通过比较解码器的name来查找

AVCodec *avcodec_find_encoder(enum CodecID id);
// 通过code ID查找一个已经注册的音视频编码器
// 引入 #include "libavcodec/avcodec.h"
// 实现在: \ffmpeg\libavcodec\utils.c
// 查找成功返回编码器指针,否则返回NULL
// 音视频编码器保存在一个链表中,查找过程中,函数从头到尾遍历链表,通过比较编码器的ID来查找

AVCodec *avcodec_find_encoder_by_name(const char *name);
// 通过一个指定的名称查找一个已经注册的音视频编码器
// 引入 #include "libavcodec/avcodec.h"
// 实现在: \ffmpeg\libavcodec\utils.c
// 查找成功返回编码器指针,否则返回NULL
// 音视频编码器保存在一个链表中,查找过程中,函数从头到尾遍历链表,通过比较编码器的名称来查找

//////////////////////////函数说明//////////////////////////







libswscale图像格式转换与放大缩小
简单的初始化方法
（1）       sws_getContext()：使用参数初始化SwsContext结构体。
（2）       sws_scale()：转换一帧图像。
（3）       sws_freeContext()：释放SwsContext结构体。
其中sws_getContext()也可以用sws_getCachedContext()取代。
更灵活的初始化方法，可以配置更多的参数：
（1）       sws_alloc_context()：为SwsContext结构体分配内存。
（2）       av_opt_set_XXX()：通过av_opt_set_int()，av_opt_set()…等一系列方法设置SwsContext结构体的值。在这里需要注意，SwsContext结构体的定义看不到，所以不能对其中的成员变量直接进行赋值，必须通过av_opt_set()这个API才能对其进行赋值。
（3）       sws_init_context()：初始化SwsContext结构体。
这种复杂的方法可以配置一些sws_getContext()配置不了的参数。
比如说设置图像的YUV像素的取值范围是
JPEG标准（Y、U、V取值范围都是0-255）
还是MPEG标准（Y取值范围是16-235，U、V的取值范围是16-240）。
可以通过使用av_opt_set()来设置“src_range”和“dst_range”输入和输出的YUV的取值范围。如果“dst_range”字段设置为“1”的话，则代表输出的YUV的取值范围遵循“jpeg”标准；如果“dst_range”字段设置为“0”的话，则代表输出的YUV的取值范围遵循“mpeg”标准。
如:
SwsContext *img_convert_ctx = sws_alloc_context();  
av_opt_set_int(img_convert_ctx,"sws_flags",SWS_BICUBIC|SWS_PRINT_INFO,0);  
av_opt_set_int(img_convert_ctx,"srcw",src_w,0);  
av_opt_set_int(img_convert_ctx,"srch",src_h,0);  
av_opt_set_int(img_convert_ctx,"src_format",src_pixfmt,0);  
av_opt_set_int(img_convert_ctx,"src_range",1,0);  
av_opt_set_int(img_convert_ctx,"dstw",dst_w,0);  
av_opt_set_int(img_convert_ctx,"dsth",dst_h,0);  
av_opt_set_int(img_convert_ctx,"dst_format",dst_pixfmt,0);  
av_opt_set_int(img_convert_ctx,"dst_range",1,0);  
sws_init_context(img_convert_ctx,NULL,NULL);  
算法性能测试：
缩小：
SWS_POINT   每秒钟可缩放约427次 效率之高，让我震撼，但效果却不差。
SWS_FAST_BILINEAR   228次
放大
SWS_POINT  112次，边缘有明显锯齿
SWS_FAST_BILINEAR 103次，效果不错
建议:
在不明确是放大还是缩小时，直接使用SWS_FAST_BILINEAR算法即可。
如果明确是要缩小并显示，建议使用Point算法。
FFmpeg使用不同sws_scale()缩放算法的命令示例
	ffmpeg -s 480x272 -pix_fmt yuv420p -i src01_480x272.yuv -s 1280x720 -sws_flags bilinear -pix_fmt yuv420p src01_bilinear_1280x720.yuv  
	ffmpeg -s 480x272 -pix_fmt yuv420p -i src01_480x272.yuv -s 1280x720 -sws_flags bicubic -pix_fmt yuv420p src01_bicubic_1280x720.yuv  
	ffmpeg -s 480x272 -pix_fmt yuv420p -i src01_480x272.yuv -s 1280x720 -sws_flags neighbor -pix_fmt yuv420p src01_neighbor_1280x720.yuv  

像素格式
（1）       所有的像素格式的名称都是以“AV_PIX_FMT_”开头
（2）       像素格式名称后面有“P”的，代表是planar格式，否则就是packed格式。Planar格式不同的分量分别存储在不同的数组中，例如AV_PIX_FMT_YUV420P存储方式如下：
data[0]: Y1, Y2, Y3, Y4, Y5, Y6, Y7, Y8……
data[1]: U1, U2, U3, U4……
data[2]: V1, V2, V3, V4……
Packed格式的数据都存储在同一个数组中，例如AV_PIX_FMT_RGB24存储方式如下：
data[0]: R1, G1, B1, R2, G2, B2, R3, G3, B3, R4, G4, B4……
（3）       像素格式名称后面有“BE”的，代表是Big Endian格式；名称后面有“LE”的，代表是Little Endian格式。

图像拉伸
FFmpeg支持多种像素拉伸的方式。这些方式的定义位于libswscale\swscale.h中，如下所示。
#define SWS_FAST_BILINEAR     1
#define SWS_BILINEAR          2
#define SWS_BICUBIC           4
#define SWS_X                 8
#define SWS_POINT          0x10
#define SWS_AREA           0x20
#define SWS_BICUBLIN       0x40
#define SWS_GAUSS          0x80
#define SWS_SINC          0x100
#define SWS_LANCZOS       0x200
#define SWS_SPLINE        0x400
其中SWS_BICUBIC性能比较好;
SWS_FAST_BILINEAR在性能和速度之间有一个比较好的平衡;
而SWS_POINT的效果比较差.

YUV像素取值范围
FFmpeg中可以通过使用av_opt_set()设置“src_range”和“dst_range”来设置输入和输出的YUV的取值范围。如果“dst_range”字段设置为“1”的话，则代表输出的YUV的取值范围遵循“jpeg”标准；如果“dst_range”字段设置为“0”的话，则代表输出的YUV的取值范围遵循“mpeg”标准。
与RGB每个像素点的每个分量取值范围为0-255不同（每个分量占8bit），YUV取值范围有两种：
（1）       以Rec.601为代表（还包括BT.709 / BT.2020）的广播电视标准中，
					Y的取值范围是16-235，U、V的取值范围是16-240。FFmpeg中称之为“mpeg”范围。
（2）       以JPEG为代表的标准中，Y、U、V的取值范围都是0-255。FFmpeg中称之为“jpeg” 范围。
实际中最常见的是第1种取值范围的YUV（可以自己观察一下YUV的数据，会发现其中亮度分量没有取值为0、255这样的数值）。

FFMPEG实现YUV，RGB各种图像原始数据之间的转换（swscale）
swscale主要用于在2个AVFrame之间进行转换。
AVFrame *srcAVFrame = NULL, *dstAVFrame = NULL;
// 申请内存
srcAVFrame = av_frame_alloc();
dstAVFrame = av_frame_alloc();
// 使用过程
av_read_frame(avFormatContext, avPacket);
result = avcodec_decode_video2(videoAVCodecContext, srcAVFrame, &got_picture_ptr, avPacket);
sws_scale(swsContext,
          (const uint8_t *const *) srcAVFrame->data,
          srcAVFrame->linesize,
          0,
          screen_h,
          dstAVFrame->data,
          dstAVFrame->linesize);
// 释放内存
av_frame_free(&srcAVFrame);
av_frame_free(&dstAVFrame);
从代码中可以看出，解码后的视频帧数据保存在srcAVFrame变量中，然后经过swscale函数转换后，将视频帧数据保存在dstAVFrame变量中。最后将dstAVFrame中的数据写入成文件。
最终数据想要保存成什么样的格式,在初始化的时候需要这样做:
1.
struct SwsContext *swsContext = 
sws_getContext(screen_w, screen_h,
                videoAVCodecContext->pix_fmt,
                screen_w, screen_h,
                AV_PIX_FMT_YUV420P,// 想要转换成什么样的格式
                SWS_BICUBIC,// SWS_POINT
                NULL, NULL, NULL);
2.
uint8_t *out_buffer = 
(uint8_t *) av_malloc(avpicture_get_size(AV_PIX_FMT_YUV420P, // 想要转换成什么样的格式
                                          screen_w, screen_h));
3.
avpicture_fill((AVPicture *) dstAVFrame,
                   out_buffer,
                   AV_PIX_FMT_YUV420P,// 想要转换成什么样的格式
                   screen_w,
                   screen_h);

最后，如果想将转换后的原始数据存成文件，只需要将pFrameYUV的data指针指向的数据写入文件就可以了。
例如，保存YUV420P格式的数据，用以下代码：
fwrite(pFrameYUV->data[0],(pCodecCtx->width)*(pCodecCtx->height),1,output);
fwrite(pFrameYUV->data[1],(pCodecCtx->width)*(pCodecCtx->height)/4,1,output);
fwrite(pFrameYUV->data[2],(pCodecCtx->width)*(pCodecCtx->height)/4,1,output);
保存RGB24格式的数据，用以下代码：
fwrite(pFrameYUV->data[0],(pCodecCtx->width)*(pCodecCtx->height)*3,1,output);
保存UYVY格式的数据，用以下代码：
fwrite(pFrameYUV->data[0],(pCodecCtx->width)*(pCodecCtx->height),2,output);
在这里又有一个问题，YUV420P格式需要写入data[0]，data[1]，data[2]；而RGB24，UYVY格式却仅仅是写入data[0]，他们的区别到底是什么呢？经过研究发现，在FFMPEG中，图像原始数据包括两种：planar和packed。planar就是将几个分量分开存，比如YUV420中，data[0]专门存Y，data[1]专门存U，data[2]专门存V。而packed则是打包存，所有数据都存在data[0]中。

avcodec_decode_video2()解码视频后丢帧的问题解决
int skipped_frame = 0;
while (av_read_frame(ifmt_ctx, &packet) >= 0) {
    ret = avcodec_decode_video2(video_dec_ctx, vframe, &got_frame, &packet);
    if (got_frame) {
        packet.pts = av_rescale_q(packet.pts, video_dec_st->time_base, video_enc_st->time_base);
        write_video_frame(ofmt_ctx, video_enc_st, vframe);
    } else {
        skipped_frame++;
    }
}
for (int i = skipped_frame; i > 0; i--) {
    ret = avcodec_decode_video2(video_dec_ctx, vframe, &got_frame, &packet);
    if (got_frame) {
        packet.pts = av_rescale_q(packet.pts, video_dec_st->time_base, video_enc_st->time_base);
        write_video_frame(ofmt_ctx, video_enc_st, vframe);
    }
}

avcodec_decode_video2使用参数picture注意点
一句话:
解码时,当AVCodecContext的refcounted_frames字段为0,
则frame的分配与释放由ffmpeg内部自己控制.
那么使用是要注意不要破坏picture的数据指针.


























































































