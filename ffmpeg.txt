https://segmentfault.com/p/1210000017101819/read

//////////////////////////函数说明//////////////////////////

pts: AV_NOPTS_VALUE
dts: AV_NOPTS_VALUE
pos: 0

int av_parser_parse2(AVCodecParserContext *s,
                     AVCodecContext *avctx,
                     uint8_t **poutbuf, int *poutbuf_size,
                     const uint8_t *buf, int buf_size,
                     int64_t pts, int64_t dts,
                     int64_t pos);

使用AVCodecParser从输入的数据流中分离出一帧一帧的压缩编码数据。
AVCodecParser用于解析输入的数据流并把它们分成一帧一帧的压缩编码数据。
比较形象的说法就是把长长的一段连续的数据“切割”成一段段的数据。
核心函数是av_parser_parse2().
解析数据获得一个Packet， 从输入的数据流中分离出一帧一帧的压缩编码数据。
通过av_parser_parse2拿到AVPaket数据，跟av_read_frame类似.
输入必须是只包含视频编码数据“裸流”（例如H.264、HEVC码流文件），
而不能是包含封装格式的媒体数据（例如AVI、MKV、MP4）.

AVCodec *avcodec_find_decoder(enum AVCodecID id);
// 通过code ID查找一个已经注册的音视频解码器
// 引入 #include "libavcodec/avcodec.h"
// 实现在: \ffmpeg\libavcodec\utils.c
// 查找成功返回解码器指针,否则返回NULL
// 音视频解码器保存在一个链表中,查找过程中,函数从头到尾遍历链表,通过比较解码器的ID来查找

AVCodec *avcodec_find_decoder_by_name(const char *name);
// 通过一个指定的名称查找一个已经注册的音视频解码器
// 引入 #include "libavcodec/avcodec.h"
// 实现在: \ffmpeg\libavcodec\utils.c
// 查找成功返回解码器指针,否则返回NULL
// 音视频解码器保存在一个链表中,查找过程中,函数从头到尾遍历链表,通过比较解码器的name来查找

AVCodec *avcodec_find_encoder(enum CodecID id);
// 通过code ID查找一个已经注册的音视频编码器
// 引入 #include "libavcodec/avcodec.h"
// 实现在: \ffmpeg\libavcodec\utils.c
// 查找成功返回编码器指针,否则返回NULL
// 音视频编码器保存在一个链表中,查找过程中,函数从头到尾遍历链表,通过比较编码器的ID来查找

AVCodec *avcodec_find_encoder_by_name(const char *name);
// 通过一个指定的名称查找一个已经注册的音视频编码器
// 引入 #include "libavcodec/avcodec.h"
// 实现在: \ffmpeg\libavcodec\utils.c
// 查找成功返回编码器指针,否则返回NULL
// 音视频编码器保存在一个链表中,查找过程中,函数从头到尾遍历链表,通过比较编码器的名称来查找

//////////////////////////函数说明//////////////////////////







libswscale图像格式转换与放大缩小
简单的初始化方法
（1）       sws_getContext()：使用参数初始化SwsContext结构体。
（2）       sws_scale()：转换一帧图像。
（3）       sws_freeContext()：释放SwsContext结构体。
其中sws_getContext()也可以用sws_getCachedContext()取代。
更灵活的初始化方法，可以配置更多的参数：
（1）       sws_alloc_context()：为SwsContext结构体分配内存。
（2）       av_opt_set_XXX()：通过av_opt_set_int()，av_opt_set()…等一系列方法设置SwsContext结构体的值。在这里需要注意，SwsContext结构体的定义看不到，所以不能对其中的成员变量直接进行赋值，必须通过av_opt_set()这个API才能对其进行赋值。
（3）       sws_init_context()：初始化SwsContext结构体。
这种复杂的方法可以配置一些sws_getContext()配置不了的参数。
比如说设置图像的YUV像素的取值范围是
JPEG标准（Y、U、V取值范围都是0-255）
还是MPEG标准（Y取值范围是16-235，U、V的取值范围是16-240）。
可以通过使用av_opt_set()来设置“src_range”和“dst_range”输入和输出的YUV的取值范围。如果“dst_range”字段设置为“1”的话，则代表输出的YUV的取值范围遵循“jpeg”标准；如果“dst_range”字段设置为“0”的话，则代表输出的YUV的取值范围遵循“mpeg”标准。
如:
SwsContext *img_convert_ctx = sws_alloc_context();  
av_opt_set_int(img_convert_ctx,"sws_flags",SWS_BICUBIC|SWS_PRINT_INFO,0);  
av_opt_set_int(img_convert_ctx,"srcw",src_w,0);  
av_opt_set_int(img_convert_ctx,"srch",src_h,0);  
av_opt_set_int(img_convert_ctx,"src_format",src_pixfmt,0);  
av_opt_set_int(img_convert_ctx,"src_range",1,0);  
av_opt_set_int(img_convert_ctx,"dstw",dst_w,0);  
av_opt_set_int(img_convert_ctx,"dsth",dst_h,0);  
av_opt_set_int(img_convert_ctx,"dst_format",dst_pixfmt,0);  
av_opt_set_int(img_convert_ctx,"dst_range",1,0);  
sws_init_context(img_convert_ctx,NULL,NULL);  
算法性能测试：
缩小：
SWS_POINT   每秒钟可缩放约427次 效率之高，让我震撼，但效果却不差。
SWS_FAST_BILINEAR   228次
放大
SWS_POINT  112次，边缘有明显锯齿
SWS_FAST_BILINEAR 103次，效果不错
建议:
在不明确是放大还是缩小时，直接使用SWS_FAST_BILINEAR算法即可。
如果明确是要缩小并显示，建议使用Point算法。
FFmpeg使用不同sws_scale()缩放算法的命令示例
	ffmpeg -s 480x272 -pix_fmt yuv420p -i src01_480x272.yuv -s 1280x720 -sws_flags bilinear -pix_fmt yuv420p src01_bilinear_1280x720.yuv  
	ffmpeg -s 480x272 -pix_fmt yuv420p -i src01_480x272.yuv -s 1280x720 -sws_flags bicubic -pix_fmt yuv420p src01_bicubic_1280x720.yuv  
	ffmpeg -s 480x272 -pix_fmt yuv420p -i src01_480x272.yuv -s 1280x720 -sws_flags neighbor -pix_fmt yuv420p src01_neighbor_1280x720.yuv  

像素格式
（1）       所有的像素格式的名称都是以“AV_PIX_FMT_”开头
（2）       像素格式名称后面有“P”的，代表是planar格式，否则就是packed格式。Planar格式不同的分量分别存储在不同的数组中，例如AV_PIX_FMT_YUV420P存储方式如下：
data[0]: Y1, Y2, Y3, Y4, Y5, Y6, Y7, Y8……
data[1]: U1, U2, U3, U4……
data[2]: V1, V2, V3, V4……
Packed格式的数据都存储在同一个数组中，例如AV_PIX_FMT_RGB24存储方式如下：
data[0]: R1, G1, B1, R2, G2, B2, R3, G3, B3, R4, G4, B4……
（3）       像素格式名称后面有“BE”的，代表是Big Endian格式；名称后面有“LE”的，代表是Little Endian格式。

图像拉伸
FFmpeg支持多种像素拉伸的方式。这些方式的定义位于libswscale\swscale.h中，如下所示。
#define SWS_FAST_BILINEAR     1
#define SWS_BILINEAR          2
#define SWS_BICUBIC           4
#define SWS_X                 8
#define SWS_POINT          0x10
#define SWS_AREA           0x20
#define SWS_BICUBLIN       0x40
#define SWS_GAUSS          0x80
#define SWS_SINC          0x100
#define SWS_LANCZOS       0x200
#define SWS_SPLINE        0x400
其中SWS_BICUBIC性能比较好;
SWS_FAST_BILINEAR在性能和速度之间有一个比较好的平衡;
而SWS_POINT的效果比较差.

YUV像素取值范围
FFmpeg中可以通过使用av_opt_set()设置“src_range”和“dst_range”来设置输入和输出的YUV的取值范围。如果“dst_range”字段设置为“1”的话，则代表输出的YUV的取值范围遵循“jpeg”标准；如果“dst_range”字段设置为“0”的话，则代表输出的YUV的取值范围遵循“mpeg”标准。
与RGB每个像素点的每个分量取值范围为0-255不同（每个分量占8bit），YUV取值范围有两种：
（1）       以Rec.601为代表（还包括BT.709 / BT.2020）的广播电视标准中，
					Y的取值范围是16-235，U、V的取值范围是16-240。FFmpeg中称之为“mpeg”范围。
（2）       以JPEG为代表的标准中，Y、U、V的取值范围都是0-255。FFmpeg中称之为“jpeg” 范围。
实际中最常见的是第1种取值范围的YUV（可以自己观察一下YUV的数据，会发现其中亮度分量没有取值为0、255这样的数值）。

FFMPEG实现YUV，RGB各种图像原始数据之间的转换（swscale）
swscale主要用于在2个AVFrame之间进行转换。
AVFrame *srcAVFrame = NULL, *dstAVFrame = NULL;
// 申请内存
srcAVFrame = av_frame_alloc();
dstAVFrame = av_frame_alloc();
// 使用过程
av_read_frame(avFormatContext, avPacket);
result = avcodec_decode_video2(videoAVCodecContext, srcAVFrame, &got_picture_ptr, avPacket);
sws_scale(swsContext,
          (const uint8_t *const *) srcAVFrame->data,
          srcAVFrame->linesize,
          0,
          screen_h,
          dstAVFrame->data,
          dstAVFrame->linesize);
// 释放内存
av_frame_free(&srcAVFrame);
av_frame_free(&dstAVFrame);
从代码中可以看出，解码后的视频帧数据保存在srcAVFrame变量中，然后经过swscale函数转换后，将视频帧数据保存在dstAVFrame变量中。最后将dstAVFrame中的数据写入成文件。
最终数据想要保存成什么样的格式,在初始化的时候需要这样做:
1.
struct SwsContext *swsContext = 
sws_getContext(screen_w, screen_h,
                videoAVCodecContext->pix_fmt,
                screen_w, screen_h,
                AV_PIX_FMT_YUV420P,// 想要转换成什么样的格式
                SWS_BICUBIC,// SWS_POINT
                NULL, NULL, NULL);
2.
uint8_t *out_buffer = 
(uint8_t *) av_malloc(avpicture_get_size(AV_PIX_FMT_YUV420P, // 想要转换成什么样的格式
                                          screen_w, screen_h));
3.
avpicture_fill((AVPicture *) dstAVFrame,
                   out_buffer,
                   AV_PIX_FMT_YUV420P,// 想要转换成什么样的格式
                   screen_w,
                   screen_h);

最后，如果想将转换后的原始数据存成文件，只需要将pFrameYUV的data指针指向的数据写入文件就可以了。
例如，保存YUV420P格式的数据，用以下代码：
fwrite(pFrameYUV->data[0],(pCodecCtx->width)*(pCodecCtx->height),1,output);
fwrite(pFrameYUV->data[1],(pCodecCtx->width)*(pCodecCtx->height)/4,1,output);
fwrite(pFrameYUV->data[2],(pCodecCtx->width)*(pCodecCtx->height)/4,1,output);
保存RGB24格式的数据，用以下代码：
fwrite(pFrameYUV->data[0],(pCodecCtx->width)*(pCodecCtx->height)*3,1,output);
保存UYVY格式的数据，用以下代码：
fwrite(pFrameYUV->data[0],(pCodecCtx->width)*(pCodecCtx->height),2,output);
在这里又有一个问题，YUV420P格式需要写入data[0]，data[1]，data[2]；而RGB24，UYVY格式却仅仅是写入data[0]，他们的区别到底是什么呢？经过研究发现，在FFMPEG中，图像原始数据包括两种：planar和packed。planar就是将几个分量分开存，比如YUV420中，data[0]专门存Y，data[1]专门存U，data[2]专门存V。而packed则是打包存，所有数据都存在data[0]中。

avcodec_decode_video2()解码视频后丢帧的问题解决
int skipped_frame = 0;
while (av_read_frame(ifmt_ctx, &packet) >= 0) {
    ret = avcodec_decode_video2(video_dec_ctx, vframe, &got_frame, &packet);
    if (got_frame) {
        packet.pts = av_rescale_q(packet.pts, video_dec_st->time_base, video_enc_st->time_base);
        write_video_frame(ofmt_ctx, video_enc_st, vframe);
    } else {
        skipped_frame++;
    }
}
for (int i = skipped_frame; i > 0; i--) {
    ret = avcodec_decode_video2(video_dec_ctx, vframe, &got_frame, &packet);
    if (got_frame) {
        packet.pts = av_rescale_q(packet.pts, video_dec_st->time_base, video_enc_st->time_base);
        write_video_frame(ofmt_ctx, video_enc_st, vframe);
    }
}

avcodec_decode_video2使用参数picture注意点
一句话:
解码时,当AVCodecContext的refcounted_frames字段为0,
则frame的分配与释放由ffmpeg内部自己控制.
那么使用是要注意不要破坏picture的数据指针.

// encoder
result = avcodec_send_frame(audioAVCodecContext, frame);
result = avcodec_receive_packet(audioAVCodecContext, avPacket);

// decoder
result = avcodec_send_packet(audioAVCodecContext, avPacket);
result = avcodec_receive_frame(audioAVCodecContext, decoded_frame);


cd x264
CC=cl ./configure --enable-static --enable-shared --enable-pic


 ./configure --toolchain=msvc --enable-yasm --enable-asm --enable-gpl --enable-libx264
 --extra-cflags=-I/usr/local/include
 --extra-ldflags=-LIBPATH:/usr/local/lib



 ./configure --prefix=/root/mydev/tools/ffmpeg --enable-libmp3lame --enable-static --enable-shared --enable-x86asm --enable-asm --enable-gpl --enable-libx264

 ffmpeg提供了av_rescale_q_rnd函数进行转换。
av_rescale_q_rnd(int64_t a, int64_t b, int64_t c, enum AVRounding rnd)
此函数主要用于对于不同时间戳的转换。具体来说是将原来以 "时间基b" 表示的 数值a 转换成以 "时间基c" 来表示的新值。AVRounding表示取整的策略

ffmpeg -i /media/1.WAV -acodec libmp3lame /media/1.MP3
ffmpeg -i apple.mp4 -f mp3 -vn apple.mp3
参数解释：
-i 表示input,即输入文件
-f 表示format,即输出格式
-vn表示vedio not,即输出不包含视频
对比源视频文件和提取得到的音频文件大小,可以看到源视频文件为约23M,而提取出来的音频文件大小为3M。

FFmpeg还提供了很多有用的工具可以查看和处理音视频文件,如：
查看视频文件的音视频编解码格式,视频时长,比特率等,如下：
dennis@ubuntu:~$ ffmpeg -i apple.mp4

ffmpeg -i test.mp4 -vcodec libx264 -b:v 1200k -r 25 -acodec mp3 -ab 128k -ar 44100 output.mp4

ffmpeg -codecs
ffmpeg -codecs | grep aac

[whb@jcwkyl introduction_to_algorithm]$ ffmpeg -i Lecture_1.flv -f mp2 -vn Lecture_1.mp3
这条命令中,-i表示input file,-f表示输出格式,-vn表示“vedio not",即禁止视频输出,最后转换后的文件是Lecture_1.mp3。
转换完成后,使用file命令查看Lecture_1.mp3的文件格式：
[whb@jcwkyl introduction_to_algorithm]$ file Lecture_1.mp3
Lecture_1.mp3: MPEG ADTS, layer II, v2,  64 kBits, 22.05 kHz, Stereo
转换前后文件大小对比：
[whb@jcwkyl introduction_to_algorithm]$ du -hs Lecture_1.*
153M    Lecture_1.flv
37M     Lecture_1.mp3
使用播放器播放Lecture_1.mp3,完全正常。

avCodec = avcodec_find_encoder_by_name("libfdk_aac");

ffmpeg中的一些参数：
   sample_format    音频采样格式
   sample_rate      采样率值(非编码值)
   channel_layout   解码后的PCM数据layout格式,左左左右右右   左右左右左右
   nb_samples       一帧音频中的采样个数,用于计算一帧数据大小

注意ffmpeg中的两个结构,AVPacket(编码的音视频帧),AVFrame(解码后的音视频数据)
AVPacket packet = {0};
packet.data =(uint8_t*)buf;
    packet.size = len;
AVFrame *decode_frame = avcodec_alloc_frame();
if(decode_frame == NULL)
{
return -1;
}
//前面已经初始化过解码器
int audio4_decode_len = avcodec_decode_audio4(av_codec_ctx_,decode_frame,got_frame,&packet);//解码
if(audio4_decode_len < 0||*got_frame != 1)
{
return -1;
}
AVSampleFormat out_sample_format = AV_SAMPLE_FMT_S16;
struct SwrContext *audio_convert_ctx;
        audio_convert_ctx = swr_alloc();
unsigned long long in_channel_layout = av_get_default_channel_layout(av_codec_ctx_->channels);
                    audio_convert_ctx = swr_alloc_set_opts(audio_convert_ctx,in_channel_layout,out_sample_format,av_codec_ctx_->sample_rate,
                                    in_channel_layout,av_codec_ctx_->sample_fmt,av_codec_ctx_->sample_rate,0,NULL);
swr_init(audio_convert_ctx);                   
    int nb_samples = swr_convert(audio_convert_ctx,//重采样,返回的值是新的采样率的一帧中的采样个数,可以根据声道和采样bit数目计算一帧数据大小
                       (uint8_t **)&outbuf,
                       AVCODEC_MAX_AUDIO_FRAME_SIZE,
                       (const uint8_t **)decode_frame->data,
                       decode_frame->nb_samples);
outlen = av_samples_get_buffer_size(NULL,av_codec_ctx_->channels ,nb_samples,out_sample_format, 1);//计算出重采样后一帧pcm数据的大小
sample_rate = av_codec_ctx_->sample_rate;
channels = av_codec_ctx_->channels;
avcodec_free_frame(&decode_frame);

音频格式的plane概念(平面)
enum AVSampleFormat {
    AV_SAMPLE_FMT_NONE = -1,
    AV_SAMPLE_FMT_U8,          ///< unsigned 8 bits
    AV_SAMPLE_FMT_S16,         ///< signed 16 bits
    AV_SAMPLE_FMT_S32,         ///< signed 32 bits
    AV_SAMPLE_FMT_FLT,         ///< float
    AV_SAMPLE_FMT_DBL,         ///< double

  // 以下都是带平面格式(带P的都是分片的)
    AV_SAMPLE_FMT_U8P,         ///< unsigned 8 bits, planar
    AV_SAMPLE_FMT_S16P,        ///< signed 16 bits, planar
    AV_SAMPLE_FMT_S32P,        ///< signed 32 bits, planar
    AV_SAMPLE_FMT_FLTP,        ///< float, planar
    AV_SAMPLE_FMT_DBLP,        ///< double, planar

    AV_SAMPLE_FMT_NB           ///< Number of sample formats. DO NOT USE if linking dynamically
};
同样对双声道音频PCM数据,以S16P为例,存储就可能是
plane 0: LLLLLLLLLLLLLLLLLLLLLLLLLL...
plane 1: RRRRRRRRRRRRRRRRRRRRRRRRRR...
而不再是以前的连续buffer。

mp3编码明确规定了只使用平面格式的数据
AAC编码依旧使用AV_SAMPLE_FMT_S16格式
输入也可能是分平面的
存储PCM数据,注意：swr_context即使进行了转换,也要判断转换后的数据是否分平面
av_sample_fmt_is_planar(pCodecCtx->sample_fmt)
编码格式要求是分平面数据
对于解码也可能需要做swr_convert,比如做播放器,很多时候我们是将S16格式数据丢给声卡,而新版ffmpeg解码音频输出的格式可能不满足S16,如AAC解码后得到的是FLT(浮点型),AC3解码是FLTP(带平面)等,需要根据具体的情况决定是否需要convert

假设现在有2个通道channel1, channel2.
那么AV_SAMPLE_FMT_S16在内存的格式就为: c1, c2, c1, c2, c1, c2, ....
而AV_SAMPLE_FMT_S16P在内存的格式为: c1, c1, c1,... c2, c2, c2,...

关于音频分片的问题
  1：无论是不是分片的数据总量是相同的.
  2：分片的存储在内存中linesize如果两声道则左右分开占用linesize[0]和linesize[1].
  3：不是分片的存储在内存中两声道不分开,左右左右....这样存储,只占用linesize[0].
音频信息
  switch (id) {
  case AV_CODEC_ID_ADPCM_ADX:    return   32;
  case AV_CODEC_ID_ADPCM_IMA_QT: return   64;
  case AV_CODEC_ID_ADPCM_EA_XAS: return  128;
  case AV_CODEC_ID_AMR_NB:
  case AV_CODEC_ID_EVRC:
  case AV_CODEC_ID_GSM:
  case AV_CODEC_ID_QCELP:
  case AV_CODEC_ID_RA_288:       return  160;
  case AV_CODEC_ID_AMR_WB:
  case AV_CODEC_ID_GSM_MS:       return  320;
  case AV_CODEC_ID_MP1:          return  384;
  case AV_CODEC_ID_ATRAC1:       return  512;
  case AV_CODEC_ID_ATRAC3:       return 1024 * framecount;
  case AV_CODEC_ID_ATRAC3P:      return 2048;
  case AV_CODEC_ID_MP2:
  case AV_CODEC_ID_MUSEPACK7:    return 1152;
  case AV_CODEC_ID_AC3:          return 1536;
  }
  AAC格式nb_samples和frame_size是1024

  如果音频,样本：s16;采样率：44100；声道：2。
  av_get_bytes_per_sample(s16) == 2;
  1：假设从麦克风或者文件读出来的通过av_read_frame得到一个数据总量是88200个字节。
     这个88200个字节是和帧无关的数据量。
  2:如果接下来需要将这些数据编码成(无论要编码成AAC还MP3都需要用到ffmpeg的fifo或者AVAudioFifo做数据缓冲)：
    1) AAC:
    nb_samples和frame_size = 1024
    一帧数据量：1024*2*av_get_bytes_per_sample(s16) = 4096个字节。
    编码：88200/(1024*2*av_get_bytes_per_sample(s16)) = 21.5帧数据
    2) MP3:
    nb_samples和frame_size = 1152
    一帧数据量：1152*2*av_get_bytes_per_sample(s16) = 4608个字节。
    编码：88200/(1152*2*av_get_bytes_per_sample(s16)) = 19.1帧数据
  3:持续时间方面
    1) AAC
    音频帧的播放时间=一个AAC帧对应的采样样本的个数/采样频率(单位为s)
    一帧 1024个 sample。采样率 Samplerate 44100KHz,每秒44100个sample, 所以根据公式   音频帧的播放时间=一个AAC帧对应的采样样本的个数/采样频率
    当前AAC一帧的播放时间是= 1024*1000000/44100= 22.2ms(单位为ms)
    2) MP3
    mp3 每帧均为1152个字节, 则：
    frame_duration = 1152 * 1000000 / sample_rate
    例如：sample_rate = 44100HZ时,计算出的时长为26.122ms,这就是经常听到的mp3每帧播放时间固定为26ms的由来

ffmpeg重采样中swr_convert和swr_get_out_samples的用法
在做mux的时候关于重采样可以用fifo,或者audiofifo做缓存处理,当做demux的时候关于重采样就可以用到上面的swr_convert和swr_get_out_samples做配合处理。
就说如果传入的nb_samles大于了传出的nb_samplse则SwrContext中会有缓存,会导致内存一直暴涨,解决方法,可以看如下代码：
  没有缓存的重采样这么处理：
      ret = swr_convert(swrcontext, pOutputFrame->data,pOutputFrame->nb_samples,
                (const uint8_t**)pInputFrame->data,pInputFrame->nb_samples);

  有缓存的代码这么处理：
      //如果还有缓存在swrcontext中,第二个参数要填写0才能获取到,缓存数据
      int fifo_size = swr_get_out_samples(swrcontext,0);
      if ( fifo_size >= pOutputFrame->nb_samples)
      {
        ret = swr_convert(swrcontext, pOutputFrame->data,pOutputFrame->nb_samples,
                  NULL,0);
      }
即如果有缓存则先判断是否有缓存在里面,如果有则传入数据为空取出缓存。


音频重采样
ffmpeg实现音频重采样的核心函数swr_convert功能非常强大,可是ffmpeg文档对它的注释太过简单,在应用中往往会出这样那样的问题,其实在读取数据->重采样->编码数据的循环中在第一次执行swr_convert后还应用swr_convert再作个缓存检测看看是否还有数据,如果有就要把它写到FIFO中去,留在下次再使用,这点在转码和由低向高转换采样率时特别重要。
  const int frame_size = FFMIN(fifo_size, m_Opt->encode_pCodecCtx->frame_size);
 
 
  if ((ret = av_audio_fifo_read(m_fifo, (void **)m_fifo_samples_array, frame_size)) < frame_size) {
    fprintf(stderr, "Could not read data from FIFO\n");
    return AVERROR_EXIT;
  }
 
  int out_samples = av_rescale_rnd(swr_get_delay(m_Opt->out_resample_context, 48000) + 1536, 44100, 48000, AV_ROUND_UP);
 
 
  int conver_samples= swr_convert(m_Opt->out_resample_context, m_fifo_conver_samples_array, frame_size,
    (const uint8_t **)m_fifo_samples_array, frame_size);
  ret = av_audio_fifo_size(m_conver_fifo);
 
  if ((ret = av_audio_fifo_realloc(m_conver_fifo, av_audio_fifo_size(m_conver_fifo) + frame_size)) < 0) {
      fprintf(stderr, "Could not reallocate FIFO\n");
      return ret;
    }
 
  ret = av_audio_fifo_write(m_conver_fifo, (void **)m_fifo_conver_samples_array, conver_samples);
    fifo_size = av_audio_fifo_size(m_conver_fifo);
 
    if ((ret = av_audio_fifo_read(m_conver_fifo, (void **)m_Opt->out_samples_array, frame_size)) < frame_size) {
      fprintf(stderr, "Could not read data from FIFO\n");
      return AVERROR_EXIT;
    }
    
    ret = Encode_audio(ret);
 
    int ret1 = 0;
 
    while ((ret1 = swr_convert(m_Opt->out_resample_context, m_fifo_conver_samples_array, frame_size, NULL, 0)) > 0)
  {
  
 
    if ((ret = av_audio_fifo_realloc(m_conver_fifo, av_audio_fifo_size(m_conver_fifo) + ret1)) < 0) {
      fprintf(stderr, "Could not reallocate FIFO\n");
      return ret;
    }
 
    ret = av_audio_fifo_write(m_conver_fifo, (void **)m_fifo_conver_samples_array, ret1);
    fifo_size = av_audio_fifo_size(m_conver_fifo);
 
    if (fifo_size > m_Opt->encode_pFrame->nb_samples)
    {
 
      if ((ret = av_audio_fifo_read(m_conver_fifo, (void **)m_Opt->out_samples_array, frame_size)) < frame_size) {
        fprintf(stderr, "Could not read data from FIFO\n");
        return AVERROR_EXIT;
      }
 
      ret = Encode_audio(ret);
 
    }

重点在：
    if ((r = swr_convert(swr_ctx, output, output_nb_samples,(const uint8_t**)input, nb_samples)) < 0)
        return -1;
 
    while ((r = swr_convert(swr_ctx, output, output_nb_samples, NULL, 0)) > 0) {
 
    }


重采样和AVAudioFifo的用法
    SwrContext * ffmpeg_init_pcm_resample(Out_stream_info * out_stream_info,AVFrame *in_frame, AVFrame *out_frame)
    {
      SwrContext * swr_ctx = NULL;  
      swr_ctx = swr_alloc();  
      if (!swr_ctx)  
      {  
        printf("swr_alloc error \n");  
        return NULL;  
      }  
      AVCodecContext * audio_dec_ctx = m_icodec->streams[m_in_audio_stream_idx]->codec;  
      AVSampleFormat sample_fmt;  
      sample_fmt = (AVSampleFormat)out_stream_info->m_dwBitsPerSample; //样本  
      int out_channel_layout = av_get_default_channel_layout(out_stream_info->m_dwChannelCount);
      if (audio_dec_ctx->channel_layout == 0)  
      {  
        audio_dec_ctx->channel_layout = av_get_default_channel_layout(m_icodec->streams[m_in_audio_stream_idx]->codec->channels);  
      }  
      /* set options */  
      av_opt_set_int(swr_ctx, "in_channel_layout",    audio_dec_ctx->channel_layout, 0);  
      av_opt_set_int(swr_ctx, "in_sample_rate",       audio_dec_ctx->sample_rate, 0);  
      av_opt_set_sample_fmt(swr_ctx, "in_sample_fmt", audio_dec_ctx->sample_fmt, 0);  
      av_opt_set_int(swr_ctx, "out_channel_layout",   out_channel_layout, 0);   
      av_opt_set_int(swr_ctx, "out_sample_rate",       out_stream_info->m_dwFrequency, 0);  
      av_opt_set_sample_fmt(swr_ctx, "out_sample_fmt", sample_fmt, 0);  
      swr_init(swr_ctx);  
     
      int64_t src_nb_samples = in_frame->nb_samples; 
      //计算输出的samples 和采样率有关 例如：48000转44100,samples则是从1152转为1059,除法
      out_frame->nb_samples = av_rescale_rnd(src_nb_samples, out_stream_info->m_dwFrequency, audio_dec_ctx->sample_rate, AV_ROUND_UP);
     
      int ret = av_samples_alloc(out_frame->data, &out_frame->linesize[0],   
        out_stream_info->m_dwChannelCount, out_frame->nb_samples,out_stream_info->m_oaudio_st->codec->sample_fmt,1);  
      if (ret < 0)  
      {  
        return NULL;  
      }  
     
      out_stream_info->m_audiofifo  = av_audio_fifo_alloc(out_stream_info->m_oaudio_st->codec->sample_fmt, out_stream_info->m_oaudio_st->codec->channels,  
        out_frame->nb_samples);   
     
      return swr_ctx;  
    }
     
    int ffmpeg_preform_pcm_resample(Out_stream_info * out_stream_info,SwrContext * pSwrCtx,AVFrame *in_frame, AVFrame *out_frame)
    {
      int ret = 0;
      int samples_out_per_size = 0;              //转换之后的samples大小
      
      if (pSwrCtx != NULL)   
      {  
        //这里注意下samples_out_per_size这个值和 out_frame->nb_samples这个值有时候不一样,ffmpeg里面做了策略不是问题。
        samples_out_per_size = swr_convert(pSwrCtx, out_frame->data, out_frame->nb_samples,   
          (const uint8_t**)in_frame->data, in_frame->nb_samples);  
        if (samples_out_per_size < 0)  
        {  
          return -1;  
        }  
     
        AVCodecContext * audio_dec_ctx = m_icodec->streams[m_in_audio_stream_idx]->codec; 
     
        int buffersize_in = av_samples_get_buffer_size(&in_frame->linesize[0],audio_dec_ctx->channels,  
          in_frame->nb_samples, audio_dec_ctx->sample_fmt, 1);
     
        //修改分包内存  
        int buffersize_out = av_samples_get_buffer_size(&out_frame->linesize[0], out_stream_info->m_oaudio_st->codec->channels,  
          samples_out_per_size, out_stream_info->m_oaudio_st->codec->sample_fmt, 1); 
     
        int fifo_size = av_audio_fifo_size(out_stream_info->m_audiofifo);  
        fifo_size = av_audio_fifo_realloc(out_stream_info->m_audiofifo, av_audio_fifo_size(out_stream_info->m_audiofifo) + out_frame->nb_samples);  
        av_audio_fifo_write(out_stream_info->m_audiofifo,(void **)out_frame->data,samples_out_per_size);  
        fifo_size = av_audio_fifo_size(out_stream_info->m_audiofifo); 
     
        out_frame->pkt_pts = in_frame->pkt_pts;  
        out_frame->pkt_dts = in_frame->pkt_dts;  
        //有时pkt_pts和pkt_dts不同,并且pkt_pts是编码前的dts,这里要给avframe传入pkt_dts而不能用pkt_pts  
        //out_frame->pts = out_frame->pkt_pts;  
        out_frame->pts = in_frame->pkt_dts;  
     
        //测试用
        if (out_stream_info->user_stream_id ==11)
        {
          if (pcm_file == NULL)
          {
            pcm_file = fopen("11.pcm","wb");
          }
          int wtiresize = fwrite(out_frame->data[0],buffersize_out,1, pcm_file);
          fflush(pcm_file);
        }
      }  
      ret = 1;
      return ret;
    }
     
    void ffmpeg_uinit_pcm_resample(SwrContext * swr_ctx,AVAudioFifo * audiofifo)
    { 
      if (swr_ctx)  
      {  
        swr_free(&swr_ctx);  
        swr_ctx = NULL;  
      }  
      if(audiofifo)  
      {  
        av_audio_fifo_free(audiofifo);  
        audiofifo = NULL;  
      }     
    }

FFmpeg关于nb_smples,frame_size以及profile的解释
原来一直记得固定编码格式需要固定的sample,例如下面：
1) AAC:
nb_samples和frame_size = 1024
一帧数据量：1024*2*av_get_bytes_per_sample(s16) = 4096个字节。
会编码：88200/(1024*2*av_get_bytes_per_sample(s16)) = 21.5帧数据
2) MP3:
nb_samples和frame_size = 1152
一帧数据量：1152*2*av_get_bytes_per_sample(s16) = 4608个字节。
MP3:则会编码：88200/(1152*2*av_get_bytes_per_sample(s16)) = 19.1帧数据

但最近发现AAC编码的音频nb_sampes和frame_size,nb_samplse是avframe中的,frame_szie是AVCodecContext中的,有可能出现2048的情况,一直以为是样本是分片planner例如AV_SAMPLE_FMT_FLT,AV_SAMPLE_FMT_FLTP这些导致,但后来发现无关。aac编码中感谢网友摘录了一些注释,如下：
    /*
    A HE-AAC v1 or v2 audio frame contains 2048 PCM samples per channel (there is
    also one mode with 1920 samples per channel but this is only for special purposes
    such as DAB+ digital radio).
    These bits/frame figures are average figures where each AAC frame generally has a different
    size in bytes. To calculate the same for AAC-LC just use 1024 instead of 2048 PCM samples per
    frame and channel.
    For AAC-LD/ELD it is either 480 or 512 PCM samples per frame and channel.
    */
从中会发现 当aac编码级别是LC时frame_size 和nb_samples是1024,如果是HE的时候是2048。

//这里的最后一个参数一定要注意用pInputFrame->nb_samples* per_sample_in,以AAC举例子,AVCodecContext中的profile会有LC,HE等不同,
//nb_samples在LC的时候是1024,在HE的时候是2048。如果不填写对会影响音频数据,nb_samples和AVCodecContext中的frame_size相同。
ret = avcodec_fill_audio_frame(pInputFrame,Channel_in,SampleFormat_in,buf_in,buf_size_in,pInputFrame->nb_samples* per_sample_in); 

其种标记在ffmpeg中是AVCodecContext中的profile：
    /**
     * profile
     * - encoding: Set by user.
     * - decoding: Set by libavcodec.
     */
     int profile;
其值如下:
#define FF_PROFILE_UNKNOWN -99
#define FF_PROFILE_RESERVED -100
 
#define FF_PROFILE_AAC_MAIN 0
#define FF_PROFILE_AAC_LOW  1
#define FF_PROFILE_AAC_SSR  2
#define FF_PROFILE_AAC_LTP  3
#define FF_PROFILE_AAC_HE   4
#define FF_PROFILE_AAC_HE_V2 28
#define FF_PROFILE_AAC_LD   22
#define FF_PROFILE_AAC_ELD  38
#define FF_PROFILE_MPEG2_AAC_LOW 128
#define FF_PROFILE_MPEG2_AAC_HE  131
 
#define FF_PROFILE_DTS         20
#define FF_PROFILE_DTS_ES      30
#define FF_PROFILE_DTS_96_24   40
#define FF_PROFILE_DTS_HD_HRA  50
#define FF_PROFILE_DTS_HD_MA   60
 
#define FF_PROFILE_MPEG2_422    0
#define FF_PROFILE_MPEG2_HIGH   1
#define FF_PROFILE_MPEG2_SS     2
#define FF_PROFILE_MPEG2_SNR_SCALABLE  3
#define FF_PROFILE_MPEG2_MAIN   4
#define FF_PROFILE_MPEG2_SIMPLE 5
 
#define FF_PROFILE_H264_CONSTRAINED  (1<<9)  // 8+1; constraint_set1_flag
#define FF_PROFILE_H264_INTRA        (1<<11) // 8+3; constraint_set3_flag
 
#define FF_PROFILE_H264_BASELINE             66
#define FF_PROFILE_H264_CONSTRAINED_BASELINE (66|FF_PROFILE_H264_CONSTRAINED)
#define FF_PROFILE_H264_MAIN                 77
#define FF_PROFILE_H264_EXTENDED             88
#define FF_PROFILE_H264_HIGH                 100
#define FF_PROFILE_H264_HIGH_10              110
#define FF_PROFILE_H264_HIGH_10_INTRA        (110|FF_PROFILE_H264_INTRA)
#define FF_PROFILE_H264_HIGH_422             122
#define FF_PROFILE_H264_HIGH_422_INTRA       (122|FF_PROFILE_H264_INTRA)
#define FF_PROFILE_H264_HIGH_444             144
#define FF_PROFILE_H264_HIGH_444_PREDICTIVE  244
#define FF_PROFILE_H264_HIGH_444_INTRA       (244|FF_PROFILE_H264_INTRA)
#define FF_PROFILE_H264_CAVLC_444            44
 
#define FF_PROFILE_VC1_SIMPLE   0
#define FF_PROFILE_VC1_MAIN     1
#define FF_PROFILE_VC1_COMPLEX  2
#define FF_PROFILE_VC1_ADVANCED 3
 
#define FF_PROFILE_MPEG4_SIMPLE                     0
#define FF_PROFILE_MPEG4_SIMPLE_SCALABLE            1
#define FF_PROFILE_MPEG4_CORE                       2
#define FF_PROFILE_MPEG4_MAIN                       3
#define FF_PROFILE_MPEG4_N_BIT                      4
#define FF_PROFILE_MPEG4_SCALABLE_TEXTURE           5
#define FF_PROFILE_MPEG4_SIMPLE_FACE_ANIMATION      6
#define FF_PROFILE_MPEG4_BASIC_ANIMATED_TEXTURE     7
#define FF_PROFILE_MPEG4_HYBRID                     8
#define FF_PROFILE_MPEG4_ADVANCED_REAL_TIME         9
#define FF_PROFILE_MPEG4_CORE_SCALABLE             10
#define FF_PROFILE_MPEG4_ADVANCED_CODING           11
#define FF_PROFILE_MPEG4_ADVANCED_CORE             12
#define FF_PROFILE_MPEG4_ADVANCED_SCALABLE_TEXTURE 13
#define FF_PROFILE_MPEG4_SIMPLE_STUDIO             14
#define FF_PROFILE_MPEG4_ADVANCED_SIMPLE           15
 
#define FF_PROFILE_JPEG2000_CSTREAM_RESTRICTION_0   0
#define FF_PROFILE_JPEG2000_CSTREAM_RESTRICTION_1   1
#define FF_PROFILE_JPEG2000_CSTREAM_NO_RESTRICTION  2
#define FF_PROFILE_JPEG2000_DCINEMA_2K              3
#define FF_PROFILE_JPEG2000_DCINEMA_4K              4 

自此,frame_size,nb_samples,profile之间的关系可以理清楚了,关于mp3的只发现过1152的,如果有其他再记录。 

音视频同步认证
杜比音效认证
美国Netflix认证

以音频为基准,同步视频到音频
1.视频慢了则加快播放或丢掉部分视频帧
2.视频快了则延迟播放,继续渲染上一帧

结构体中定义指针变量时,不需要把它置为NULL.
结构体中成员变量该定义成指针还是普通变量?
如果结构体中的成员变量是指针,那么它的值一般是其他指针变量赋值给它,
或者通过某个方法后得到一个指针.意思就是说定义成指针时,
其很容易通过某种方式给它赋值,而不需要另外分配空间后再使用.
如果结构体中的成员变量是普通变量,那么给这个结构体分配空间时,
这个普通变量也就分配了空间,可以直接使用了,只要给它一个初始值就行了.

1.
typedef struct VideoState{
  //...
} VideoState;
VideoState *is;
is = av_mallocz(sizeof(VideoState));
av_free(is);
2.
AVFrame *frame = av_frame_alloc();
if (!frame)
    return AVERROR(ENOMEM);
3.
char str[1000] = {0};
4.
// 普通数组也可以这样初始化内存
int st_index[5];
memset(st_index, -1, sizeof(st_index));
5.
结构体的变量定义,内存初始化
typedef struct PacketQueue {
  MyAVPacketList *first_pkt, *last_pkt;
  int nb_packets;
  int size;
  int64_t duration;
  int abort_request;
  int serial;
  SDL_mutex *mutex;// "同步"作用
  SDL_cond *cond;  // "暂停"作用
} PacketQueue;
PacketQueue videoq;
PacketQueue *q = &videoq;
memset(q, 0, sizeof(PacketQueue));
6.
关于SDL同步,暂停方面
SDL_mutex *mutex;// "同步"作用
SDL_cond *cond;  // "暂停"作用
// 创建
mutex = SDL_CreateMutex();
if (!mutex) {
  return AVERROR(ENOMEM);
}
cond = SDL_CreateCond();
if (!cond) {
  return AVERROR(ENOMEM);
}
// 销毁
SDL_DestroyMutex(mutex);
SDL_DestroyCond(cond);
// 使用(同步功能)
SDL_LockMutex(mutex);
ret = packet_queue_put_private(q, pkt);
SDL_UnlockMutex(mutex);
// 使用(暂停功能)
SDL_LockMutex(mutex);
SDL_CondWait(cond, mutex);
SDL_UnlockMutex(mutex);
// 使用(取消暂停功能)
SDL_LockMutex(mutex);
SDL_CondSignal(cond);
SDL_UnlockMutex(mutex);



AVFormatContext *avFormatContext = NULL;
avFormatContext = avformat_alloc_context();
if (!avFormatContext) {
  return AVERROR(ENOMEM);
}
avformat_open_input(&avFormatContext, is->filename, is->iformat, &format_opts);

AVCodecContext *avctx = NULL;
avctx = avcodec_alloc_context3(NULL);
if (!avctx)
  return AVERROR(ENOMEM);
AVCodecParameters *par = ic->streams[stream_index]->codecpar;
ret = avcodec_parameters_to_context(avctx, par);
if (ret < 0)
  goto fail;

AVCodec *deCodec = NULL;
deCodec = avcodec_find_decoder(avctx->codec_id);
if (!deCodec)
  goto fail;


audClock:
avRational = av_buffersink_get_time_base(is->out_audio_filter);
af->pts = (decodedAVFrame->pts == AV_NOPTS_VALUE) ? NAN : decodedAVFrame->pts * av_q2d(avRational);
is->audio_clock = af->pts + (double) af->frame->nb_samples / af->frame->sample_rate;
double pts = is->audio_clock -
                     (double) (2 * is->audio_hw_buf_size + is->audio_write_buf_size) / is->audio_tgt.bytes_per_sec;
set_clock_at(&is->audClock,
             pts,
             is->audio_clock_serial,
             audio_callback_time / 1000000.0);
sync_clock_to_slave(&is->extClock, &is->audClock);

Decoder audDecoder;
Decoder vidDecoder;
Decoder subDecoder;
decoder_init(&is->vidDecoder, codecContext, &is->videoPQ, &is->continue_read_thread_cond);
decoder_init(&is->audDecoder, codecContext, &is->audioPQ, &is->continue_read_thread_cond);
decoder_init(&is->subDecoder, codecContext, &is->subtitlePQ, &is->continue_read_thread_cond);
decoder_init:
  d->start_pts = AV_NOPTS_VALUE;
  d->pkt_serial = -1;
decoder_start(&is->vidDecoder, video_thread, "video_decoder", is)
decoder_start(&is->audDecoder, audio_thread, "audio_decoder", is)
decoder_start(&is->subDecoder, subtitle_thread, "subtitle_decoder", is)
got_picture = decoder_decode_frame(&is->vidDecoder, decodedAVFrame, NULL)
got_frame = decoder_decode_frame(&is->audDecoder, decodedAVFrame, NULL)
got_subtitle = decoder_decode_frame(&is->subDecoder, NULL, &sp->sub)

1.查看decoder_decode_frame方法中有没有跟Clock想着的代码?


queue_picture中有av_frame_move_ref(vp->frame, decodedAVFrame);


AVStream *audio_st = formatContext->streams[stream_index];
Decoder
  pkt_serial: 
    -1 decoder_init()
     1 packet_queue_get()
  finished:
    0  decoder_decode_frame()
  start_pts: 
    AV_NOPTS_VALUE decoder_init()
    audio_st->start_time stream_component_open() 没走到
  start_pts_tb:
    audio_st->time_base  stream_component_open() 没走到
  next_pts:
    decoder_decode_frame()在这个方法中赋值
    next_pts = start_pts 
    next_pts = decodedAVFrame->pts + decodedAVFrame->nb_samples
  next_pts_tb:
    decoder_decode_frame()
    next_pts_tb = start_pts_tb
    next_pts_tb = tb

frame_timer:
  stream_toggle_pause()
  video_refresh()

packet_queue_put()
  read_thread() is->seek_req
    packet_queue_put(&is->audioPQ, &flush_pkt)
    packet_queue_put(&is->subtitlePQ, &flush_pkt)
    packet_queue_put(&is->videoPQ, &flush_pkt)
  packet_queue_start()
    packet_queue_put_private(q, &flush_pkt)

PacketQueue:serial(在packet_queue_put_private()时加1操作)
影响
AVPacketNode:serial(packet_queue_put_private()时由PacketQueue:serial直接赋值)
Clock:queue_serial(init_clock()时传递指针,指向了PacketQueue:serial)
Decoder:pkt_serial(packet_queue_get()时传递指针,指向了AVPacketNode:serial)
Frame:serial(由Decoder:pkt_serial赋值)




































































