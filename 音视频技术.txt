Android音视频的编解码

android_atomic_inc()原子操作函数
android_atomic_add()
const_cast<RefBase*>
Android中提出了一套类似Java垃圾回收机制的智能指针,采用强指针sp(Strong Pointer)和弱指针wp(Weak Pointer)对目标对象进行应用,实现对象的自动回收。
对象可以分为全局对象、局部对象、静态全局对象和静态局部对象。
Android设计了强引用sp和弱引用wp,故实际对象的释放,可分为强引用控制和弱引用控制。所谓强引用控制,指的是强引用数mStrong为0时,释放实际对象；弱引用控制,则指的是弱引用数mWeak为0时,才释放实际对象。


// encoder
result = avcodec_send_frame(audioAVCodecContext, frame);
result = avcodec_receive_packet(audioAVCodecContext, avPacket);

// decoder
result = avcodec_send_packet(audioAVCodecContext, avPacket);
result = avcodec_receive_frame(audioAVCodecContext, decoded_frame);


cd x264
CC=cl ./configure --enable-static --enable-shared --enable-pic


 ./configure --toolchain=msvc --enable-yasm --enable-asm --enable-gpl --enable-libx264 
 --extra-cflags=-I/usr/local/include 
 --extra-ldflags=-LIBPATH:/usr/local/lib



 ./configure --prefix=/root/mydev/tools/ffmpeg --enable-libmp3lame --enable-static --enable-shared --enable-x86asm --enable-asm --enable-gpl --enable-libx264


Stream #0.0[0x1e0]: Video: mpeg2video, yuv420p, 704x576 [PAR 12:11 DAR 4:3], 9578 kb/s, 25 tbr, 90k tbn, 50 tbc
25  tbr 代表帧率
90k tbn 代表文件层的时间精度,即1S=1200k,和duration相关
50  tbc 代表视频层的时间精度,即1S=50,和strem->duration和时间戳相关

在AndroidAPI <= 20(Android5.0之前的版本)中Google支持的CameraPreview Callback的YUV常用格式有两种：一个是NV21,一个是YV12。如果我们需要对Camera采集的图像进行编码等,必须要对其进一步处理,比如格式转换、旋转等操作,否则会出现一些花屏、叠影等问题。	


采样率：每秒钟记录多少个采样点；
在H264协议里定义了三种帧,完整编码的帧叫I帧,参考之前的I帧生成的只包含差异部分编码的帧叫P帧,还有一种参考前后的帧编码的帧叫B帧。
H264采用的核心算法是帧内压缩和帧间压缩,帧内压缩是生成I帧的算法,帧间压缩是生成B帧和P帧的算法。

ffmpeg提供了av_rescale_q_rnd函数进行转换。
av_rescale_q_rnd(int64_t a, int64_t b, int64_t c, enum AVRounding rnd)
此函数主要用于对于不同时间戳的转换。具体来说是将原来以 "时间基b" 表示的 数值a 转换成以 "时间基c" 来表示的新值。AVRounding表示取整的策略

ffmpeg -i /media/1.WAV -acodec libmp3lame /media/1.MP3
ffmpeg -i apple.mp4 -f mp3 -vn apple.mp3
参数解释：
-i 表示input,即输入文件
-f 表示format,即输出格式
-vn表示vedio not,即输出不包含视频
对比源视频文件和提取得到的音频文件大小,可以看到源视频文件为约23M,而提取出来的音频文件大小为3M。

FFmpeg还提供了很多有用的工具可以查看和处理音视频文件,如：
查看视频文件的音视频编解码格式,视频时长,比特率等,如下：
dennis@ubuntu:~$ ffmpeg -i apple.mp4

ffmpeg -i test.mp4 -vcodec libx264 -b:v 1200k -r 25 -acodec mp3 -ab 128k -ar 44100 output.mp4

ffmpeg -codecs
ffmpeg -codecs | grep aac

[whb@jcwkyl introduction_to_algorithm]$ ffmpeg -i Lecture_1.flv -f mp2 -vn Lecture_1.mp3
这条命令中,-i表示input file,-f表示输出格式,-vn表示“vedio not",即禁止视频输出,最后转换后的文件是Lecture_1.mp3。
转换完成后,使用file命令查看Lecture_1.mp3的文件格式：
[whb@jcwkyl introduction_to_algorithm]$ file Lecture_1.mp3
Lecture_1.mp3: MPEG ADTS, layer II, v2,  64 kBits, 22.05 kHz, Stereo
转换前后文件大小对比：
[whb@jcwkyl introduction_to_algorithm]$ du -hs Lecture_1.*
153M    Lecture_1.flv
37M     Lecture_1.mp3
使用播放器播放Lecture_1.mp3,完全正常。



avCodec = avcodec_find_encoder_by_name("libfdk_aac");

常用的立体声有2个通道,环绕立体声3个通道。
声道数目 立体声(stero)单声道(mono)

采样率  单位时间内对音频ad芯片的采样次数,常见的音频采样率(sample_rate)有。
 0: 96000 Hz
 1: 88200 Hz
 2: 64000 Hz
 3: 48000 Hz
 4: 44100 Hz
 5: 32000 Hz
 6: 24000 Hz
 7: 22050 Hz
 8: 16000 Hz
 9: 12000 Hz
10: 11025 Hz
11: 8000 Hz
12: 7350 Hz
13: Reserved
14: Reserved
15: frequency is written explicitly
左边是adts头里面对应的采样率编码值,右边是对应的采样率值。
单采样bit数目  单个采样的bit位数,16／8 bit。

音频
32 kbps —MW(AM) 质量
96 kbps —FM质量
128 - 160 kbps –相当好的质量,有时有明显差别
192 kbps — 优良质量,偶尔有差别
224 - 320 kbps — 高质量
音频
800 bps – 能够分辨的语音所需最低码率(需使用专用的FS-1015语音编解码器)
8 kbps —电话质量(使用语音编码)
8-500 kbps --Ogg Vorbis和MPEG1 Player1/2/3中使用的有损音频模式
500 kbps–1.4 Mbps —44.1KHz的无损音频,解码器为FLAC Audio,WavPack或Monkey's Audio
1411.2 - 2822.4 Kbps —脉冲编码调制(PCM)声音格式CD光碟的数字音频
5644.8 kbps —SACD使用的Direct Stream Digital格式

ffmpeg中的一些参数：
   sample_format    音频采样格式
   sample_rate      采样率值(非编码值)
   channel_layout   解码后的PCM数据layout格式,左左左右右右   左右左右左右
   nb_samples       一帧音频中的采样个数,用于计算一帧数据大小

注意ffmpeg中的两个结构,AVPacket(编码的音视频帧),AVFrame(解码后的音视频数据)
AVPacket packet = {0};
packet.data =(uint8_t*)buf;
    packet.size = len;
AVFrame *decode_frame = avcodec_alloc_frame();
if(decode_frame == NULL)
{
return -1;
}
//前面已经初始化过解码器
int audio4_decode_len = avcodec_decode_audio4(av_codec_ctx_,decode_frame,got_frame,&packet);//解码
if(audio4_decode_len < 0||*got_frame != 1)
{
return -1;
}
AVSampleFormat out_sample_format = AV_SAMPLE_FMT_S16;
struct SwrContext *audio_convert_ctx;
        audio_convert_ctx = swr_alloc();
unsigned long long in_channel_layout = av_get_default_channel_layout(av_codec_ctx_->channels);
                    audio_convert_ctx = swr_alloc_set_opts(audio_convert_ctx,in_channel_layout,out_sample_format,av_codec_ctx_->sample_rate,
                                    in_channel_layout,av_codec_ctx_->sample_fmt,av_codec_ctx_->sample_rate,0,NULL);
swr_init(audio_convert_ctx);                   
    int nb_samples = swr_convert(audio_convert_ctx,//重采样,返回的值是新的采样率的一帧中的采样个数,可以根据声道和采样bit数目计算一帧数据大小
                       (uint8_t **)&outbuf,
                       AVCODEC_MAX_AUDIO_FRAME_SIZE,
                       (const uint8_t **)decode_frame->data,
                       decode_frame->nb_samples);
outlen = av_samples_get_buffer_size(NULL,av_codec_ctx_->channels ,nb_samples,out_sample_format, 1);//计算出重采样后一帧pcm数据的大小
sample_rate = av_codec_ctx_->sample_rate;
channels = av_codec_ctx_->channels;
avcodec_free_frame(&decode_frame);

封装格式
作用：视频码流和音频码流按照一定的格式储存在一个文件汇总
视频编码数据
作用：将视频像素数据(RGB,YUV等)压缩成为视频码流,从而降低视频的数据量
音频编码数据
作用：将音频采样数据(PCM等)压缩成为音频码流,从而降低音频的数据量
视频像素数据
作用：保存了屏幕上每一个像素点的像素值
格式：常见的像素数据格式有RGB24, RGB32, YUV420P,YUV422P,YUV444P等。压缩编码中一般使用的是YUV格式的像素数据,最为常见的格式为YUV420P。
特点：视频像素数据体积很大,一般情况下一小时高清视频的RGB24格式的数据体积为：3600*25*1920*1080*3=559.9GB(PS：这里假定帧率为25HZ,取样精度8bit)
音频采样数据
作用：保存了音频中每个采样点的值。
特点：音频采样数据体积很大,一般情况下一首4分钟的PCM格式的
   歌曲体积为:
   4*60*44100*2*2=42.3MByte
          PS:这里假定采样率为44100Hz,采样精度为16bit


ffmpeg解码
OpenGL绘图
OpenGL-PBO加速
OpenGL-Double PBO加速
YUV格式转RGB(由GPU完成)
多线程解码
多路视频
视频特效


音频格式的plane概念(平面)
enum AVSampleFormat {
    AV_SAMPLE_FMT_NONE = -1,
    AV_SAMPLE_FMT_U8,          ///< unsigned 8 bits
    AV_SAMPLE_FMT_S16,         ///< signed 16 bits
    AV_SAMPLE_FMT_S32,         ///< signed 32 bits
    AV_SAMPLE_FMT_FLT,         ///< float
    AV_SAMPLE_FMT_DBL,         ///< double

	// 以下都是带平面格式(带P的都是分片的)
    AV_SAMPLE_FMT_U8P,         ///< unsigned 8 bits, planar
    AV_SAMPLE_FMT_S16P,        ///< signed 16 bits, planar
    AV_SAMPLE_FMT_S32P,        ///< signed 32 bits, planar
    AV_SAMPLE_FMT_FLTP,        ///< float, planar
    AV_SAMPLE_FMT_DBLP,        ///< double, planar

    AV_SAMPLE_FMT_NB           ///< Number of sample formats. DO NOT USE if linking dynamically
};
同样对双声道音频PCM数据,以S16P为例,存储就可能是
plane 0: LLLLLLLLLLLLLLLLLLLLLLLLLL...
plane 1: RRRRRRRRRRRRRRRRRRRRRRRRRR...
而不再是以前的连续buffer。

mp3编码明确规定了只使用平面格式的数据
AAC编码依旧使用AV_SAMPLE_FMT_S16格式
输入也可能是分平面的
存储PCM数据,注意：swr_context即使进行了转换,也要判断转换后的数据是否分平面
av_sample_fmt_is_planar(pCodecCtx->sample_fmt)
编码格式要求是分平面数据
对于解码也可能需要做swr_convert,比如做播放器,很多时候我们是将S16格式数据丢给声卡,而新版ffmpeg解码音频输出的格式可能不满足S16,如AAC解码后得到的是FLT(浮点型),AC3解码是FLTP(带平面)等,需要根据具体的情况决定是否需要convert

假设现在有2个通道channel1, channel2.
那么AV_SAMPLE_FMT_S16在内存的格式就为: c1, c2, c1, c2, c1, c2, ....
而AV_SAMPLE_FMT_S16P在内存的格式为: c1, c1, c1,... c2, c2, c2,...

关于音频分片的问题
	1：无论是不是分片的数据总量是相同的.
	2：分片的存储在内存中linesize如果两声道则左右分开占用linesize[0]和linesize[1].
	3：不是分片的存储在内存中两声道不分开,左右左右....这样存储,只占用linesize[0].
音频信息
	switch (id) {
	case AV_CODEC_ID_ADPCM_ADX:    return   32;
	case AV_CODEC_ID_ADPCM_IMA_QT: return   64;
	case AV_CODEC_ID_ADPCM_EA_XAS: return  128;
	case AV_CODEC_ID_AMR_NB:
	case AV_CODEC_ID_EVRC:
	case AV_CODEC_ID_GSM:
	case AV_CODEC_ID_QCELP:
	case AV_CODEC_ID_RA_288:       return  160;
	case AV_CODEC_ID_AMR_WB:
	case AV_CODEC_ID_GSM_MS:       return  320;
	case AV_CODEC_ID_MP1:          return  384;
	case AV_CODEC_ID_ATRAC1:       return  512;
	case AV_CODEC_ID_ATRAC3:       return 1024 * framecount;
	case AV_CODEC_ID_ATRAC3P:      return 2048;
	case AV_CODEC_ID_MP2:
	case AV_CODEC_ID_MUSEPACK7:    return 1152;
	case AV_CODEC_ID_AC3:          return 1536;
	}
	AAC格式nb_samples和frame_size是1024

	如果音频,样本：s16;采样率：44100；声道：2。
	av_get_bytes_per_sample(s16) == 2;
	1：假设从麦克风或者文件读出来的通过av_read_frame得到一个数据总量是88200个字节。
	   这个88200个字节是和帧无关的数据量。
	2:如果接下来需要将这些数据编码成(无论要编码成AAC还MP3都需要用到ffmpeg的fifo或者AVAudioFifo做数据缓冲)：
		1) AAC:
		nb_samples和frame_size = 1024
		一帧数据量：1024*2*av_get_bytes_per_sample(s16) = 4096个字节。
		编码：88200/(1024*2*av_get_bytes_per_sample(s16)) = 21.5帧数据
		2) MP3:
		nb_samples和frame_size = 1152
		一帧数据量：1152*2*av_get_bytes_per_sample(s16) = 4608个字节。
		编码：88200/(1152*2*av_get_bytes_per_sample(s16)) = 19.1帧数据
	3:持续时间方面
		1) AAC
		音频帧的播放时间=一个AAC帧对应的采样样本的个数/采样频率(单位为s)
		一帧 1024个 sample。采样率 Samplerate 44100KHz,每秒44100个sample, 所以根据公式   音频帧的播放时间=一个AAC帧对应的采样样本的个数/采样频率
		当前AAC一帧的播放时间是= 1024*1000000/44100= 22.2ms(单位为ms)
		2) MP3
		mp3 每帧均为1152个字节, 则：
		frame_duration = 1152 * 1000000 / sample_rate
		例如：sample_rate = 44100HZ时,计算出的时长为26.122ms,这就是经常听到的mp3每帧播放时间固定为26ms的由来

ffmpeg重采样中swr_convert和swr_get_out_samples的用法
在做mux的时候关于重采样可以用fifo,或者audiofifo做缓存处理,当做demux的时候关于重采样就可以用到上面的swr_convert和swr_get_out_samples做配合处理。
就说如果传入的nb_samles大于了传出的nb_samplse则SwrContext中会有缓存,会导致内存一直暴涨,解决方法,可以看如下代码：
	没有缓存的重采样这么处理：
	    ret = swr_convert(swrcontext, pOutputFrame->data,pOutputFrame->nb_samples,
	    					(const uint8_t**)pInputFrame->data,pInputFrame->nb_samples);

	有缓存的代码这么处理：
	    //如果还有缓存在swrcontext中,第二个参数要填写0才能获取到,缓存数据
	    int fifo_size = swr_get_out_samples(swrcontext,0);
	    if ( fifo_size >= pOutputFrame->nb_samples)
	    {
	    	ret = swr_convert(swrcontext, pOutputFrame->data,pOutputFrame->nb_samples,
	    						NULL,0);
	    }
即如果有缓存则先判断是否有缓存在里面,如果有则传入数据为空取出缓存。


音频重采样
ffmpeg实现音频重采样的核心函数swr_convert功能非常强大,可是ffmpeg文档对它的注释太过简单,在应用中往往会出这样那样的问题,其实在读取数据->重采样->编码数据的循环中在第一次执行swr_convert后还应用swr_convert再作个缓存检测看看是否还有数据,如果有就要把它写到FIFO中去,留在下次再使用,这点在转码和由低向高转换采样率时特别重要。
	const int frame_size = FFMIN(fifo_size, m_Opt->encode_pCodecCtx->frame_size);
 
 
	if ((ret = av_audio_fifo_read(m_fifo, (void **)m_fifo_samples_array, frame_size)) < frame_size) {
		fprintf(stderr, "Could not read data from FIFO\n");
		return AVERROR_EXIT;
	}
 
	int out_samples = av_rescale_rnd(swr_get_delay(m_Opt->out_resample_context, 48000) + 1536, 44100, 48000, AV_ROUND_UP);
 
 
	int conver_samples= swr_convert(m_Opt->out_resample_context, m_fifo_conver_samples_array, frame_size,
		(const uint8_t **)m_fifo_samples_array, frame_size);
	ret = av_audio_fifo_size(m_conver_fifo);
 
	if ((ret = av_audio_fifo_realloc(m_conver_fifo, av_audio_fifo_size(m_conver_fifo) + frame_size)) < 0) {
			fprintf(stderr, "Could not reallocate FIFO\n");
			return ret;
		}
 
	ret = av_audio_fifo_write(m_conver_fifo, (void **)m_fifo_conver_samples_array, conver_samples);
		fifo_size = av_audio_fifo_size(m_conver_fifo);
 
		if ((ret = av_audio_fifo_read(m_conver_fifo, (void **)m_Opt->out_samples_array, frame_size)) < frame_size) {
			fprintf(stderr, "Could not read data from FIFO\n");
			return AVERROR_EXIT;
		}
		
		ret = Encode_audio(ret);
 
		int ret1 = 0;
 
		while ((ret1 = swr_convert(m_Opt->out_resample_context, m_fifo_conver_samples_array, frame_size, NULL, 0)) > 0)
	{
	
 
		if ((ret = av_audio_fifo_realloc(m_conver_fifo, av_audio_fifo_size(m_conver_fifo) + ret1)) < 0) {
			fprintf(stderr, "Could not reallocate FIFO\n");
			return ret;
		}
 
		ret = av_audio_fifo_write(m_conver_fifo, (void **)m_fifo_conver_samples_array, ret1);
		fifo_size = av_audio_fifo_size(m_conver_fifo);
 
		if (fifo_size > m_Opt->encode_pFrame->nb_samples)
		{
 
			if ((ret = av_audio_fifo_read(m_conver_fifo, (void **)m_Opt->out_samples_array, frame_size)) < frame_size) {
				fprintf(stderr, "Could not read data from FIFO\n");
				return AVERROR_EXIT;
			}
 
			ret = Encode_audio(ret);
 
		}

重点在：
    if ((r = swr_convert(swr_ctx, output, output_nb_samples,(const uint8_t**)input, nb_samples)) < 0)
        return -1;
 
    while ((r = swr_convert(swr_ctx, output, output_nb_samples, NULL, 0)) > 0) {
 
    }


重采样和AVAudioFifo的用法
    SwrContext * ffmpeg_init_pcm_resample(Out_stream_info * out_stream_info,AVFrame *in_frame, AVFrame *out_frame)
    {
    	SwrContext * swr_ctx = NULL;  
    	swr_ctx = swr_alloc();  
    	if (!swr_ctx)  
    	{  
    		printf("swr_alloc error \n");  
    		return NULL;  
    	}  
    	AVCodecContext * audio_dec_ctx = m_icodec->streams[m_in_audio_stream_idx]->codec;  
    	AVSampleFormat sample_fmt;  
    	sample_fmt = (AVSampleFormat)out_stream_info->m_dwBitsPerSample; //样本  
    	int out_channel_layout = av_get_default_channel_layout(out_stream_info->m_dwChannelCount);
    	if (audio_dec_ctx->channel_layout == 0)  
    	{  
    		audio_dec_ctx->channel_layout = av_get_default_channel_layout(m_icodec->streams[m_in_audio_stream_idx]->codec->channels);  
    	}  
    	/* set options */  
    	av_opt_set_int(swr_ctx, "in_channel_layout",    audio_dec_ctx->channel_layout, 0);  
    	av_opt_set_int(swr_ctx, "in_sample_rate",       audio_dec_ctx->sample_rate, 0);  
    	av_opt_set_sample_fmt(swr_ctx, "in_sample_fmt", audio_dec_ctx->sample_fmt, 0);  
    	av_opt_set_int(swr_ctx, "out_channel_layout",   out_channel_layout, 0);   
    	av_opt_set_int(swr_ctx, "out_sample_rate",       out_stream_info->m_dwFrequency, 0);  
    	av_opt_set_sample_fmt(swr_ctx, "out_sample_fmt", sample_fmt, 0);  
    	swr_init(swr_ctx);  
     
    	int64_t src_nb_samples = in_frame->nb_samples; 
    	//计算输出的samples 和采样率有关 例如：48000转44100,samples则是从1152转为1059,除法
    	out_frame->nb_samples = av_rescale_rnd(src_nb_samples, out_stream_info->m_dwFrequency, audio_dec_ctx->sample_rate, AV_ROUND_UP);
     
    	int ret = av_samples_alloc(out_frame->data, &out_frame->linesize[0],   
    		out_stream_info->m_dwChannelCount, out_frame->nb_samples,out_stream_info->m_oaudio_st->codec->sample_fmt,1);  
    	if (ret < 0)  
    	{  
    		return NULL;  
    	}  
     
    	out_stream_info->m_audiofifo  = av_audio_fifo_alloc(out_stream_info->m_oaudio_st->codec->sample_fmt, out_stream_info->m_oaudio_st->codec->channels,  
    		out_frame->nb_samples);   
     
    	return swr_ctx;  
    }
     
    int ffmpeg_preform_pcm_resample(Out_stream_info * out_stream_info,SwrContext * pSwrCtx,AVFrame *in_frame, AVFrame *out_frame)
    {
    	int ret = 0;
    	int samples_out_per_size = 0;              //转换之后的samples大小
      
    	if (pSwrCtx != NULL)   
    	{  
    		//这里注意下samples_out_per_size这个值和 out_frame->nb_samples这个值有时候不一样,ffmpeg里面做了策略不是问题。
    		samples_out_per_size = swr_convert(pSwrCtx, out_frame->data, out_frame->nb_samples,   
    			(const uint8_t**)in_frame->data, in_frame->nb_samples);  
    		if (samples_out_per_size < 0)  
    		{  
    			return -1;  
    		}  
     
    		AVCodecContext * audio_dec_ctx = m_icodec->streams[m_in_audio_stream_idx]->codec; 
     
    		int buffersize_in = av_samples_get_buffer_size(&in_frame->linesize[0],audio_dec_ctx->channels,  
    			in_frame->nb_samples, audio_dec_ctx->sample_fmt, 1);
     
    		//修改分包内存  
    		int buffersize_out = av_samples_get_buffer_size(&out_frame->linesize[0], out_stream_info->m_oaudio_st->codec->channels,  
    			samples_out_per_size, out_stream_info->m_oaudio_st->codec->sample_fmt, 1); 
     
    		int fifo_size = av_audio_fifo_size(out_stream_info->m_audiofifo);  
    		fifo_size = av_audio_fifo_realloc(out_stream_info->m_audiofifo, av_audio_fifo_size(out_stream_info->m_audiofifo) + out_frame->nb_samples);  
    		av_audio_fifo_write(out_stream_info->m_audiofifo,(void **)out_frame->data,samples_out_per_size);  
    		fifo_size = av_audio_fifo_size(out_stream_info->m_audiofifo); 
     
    		out_frame->pkt_pts = in_frame->pkt_pts;  
    		out_frame->pkt_dts = in_frame->pkt_dts;  
    		//有时pkt_pts和pkt_dts不同,并且pkt_pts是编码前的dts,这里要给avframe传入pkt_dts而不能用pkt_pts  
    		//out_frame->pts = out_frame->pkt_pts;  
    		out_frame->pts = in_frame->pkt_dts;  
     
    		//测试用
    		if (out_stream_info->user_stream_id ==11)
    		{
    			if (pcm_file == NULL)
    			{
    				pcm_file = fopen("11.pcm","wb");
    			}
    			int wtiresize = fwrite(out_frame->data[0],buffersize_out,1, pcm_file);
    			fflush(pcm_file);
    		}
    	}  
    	ret = 1;
    	return ret;
    }
     
    void ffmpeg_uinit_pcm_resample(SwrContext * swr_ctx,AVAudioFifo * audiofifo)
    { 
    	if (swr_ctx)  
    	{  
    		swr_free(&swr_ctx);  
    		swr_ctx = NULL;  
    	}  
    	if(audiofifo)  
    	{  
    		av_audio_fifo_free(audiofifo);  
    		audiofifo = NULL;  
    	}     
    }

FFmpeg关于nb_smples,frame_size以及profile的解释
原来一直记得固定编码格式需要固定的sample,例如下面：
1) AAC:
nb_samples和frame_size = 1024
一帧数据量：1024*2*av_get_bytes_per_sample(s16) = 4096个字节。
会编码：88200/(1024*2*av_get_bytes_per_sample(s16)) = 21.5帧数据
2) MP3:
nb_samples和frame_size = 1152
一帧数据量：1152*2*av_get_bytes_per_sample(s16) = 4608个字节。
MP3:则会编码：88200/(1152*2*av_get_bytes_per_sample(s16)) = 19.1帧数据

但最近发现AAC编码的音频nb_sampes和frame_size,nb_samplse是avframe中的,frame_szie是AVCodecContext中的,有可能出现2048的情况,一直以为是样本是分片planner例如AV_SAMPLE_FMT_FLT,AV_SAMPLE_FMT_FLTP这些导致,但后来发现无关。aac编码中感谢网友摘录了一些注释,如下：
    /*
    A HE-AAC v1 or v2 audio frame contains 2048 PCM samples per channel (there is
    also one mode with 1920 samples per channel but this is only for special purposes
    such as DAB+ digital radio).
    These bits/frame figures are average figures where each AAC frame generally has a different
    size in bytes. To calculate the same for AAC-LC just use 1024 instead of 2048 PCM samples per
    frame and channel.
    For AAC-LD/ELD it is either 480 or 512 PCM samples per frame and channel.
    */
从中会发现 当aac编码级别是LC时frame_size 和nb_samples是1024,如果是HE的时候是2048。

//这里的最后一个参数一定要注意用pInputFrame->nb_samples* per_sample_in,以AAC举例子,AVCodecContext中的profile会有LC,HE等不同,
//nb_samples在LC的时候是1024,在HE的时候是2048。如果不填写对会影响音频数据,nb_samples和AVCodecContext中的frame_size相同。
ret = avcodec_fill_audio_frame(pInputFrame,Channel_in,SampleFormat_in,buf_in,buf_size_in,pInputFrame->nb_samples* per_sample_in); 

其种标记在ffmpeg中是AVCodecContext中的profile：
    /**
     * profile
     * - encoding: Set by user.
     * - decoding: Set by libavcodec.
     */
     int profile;
其值如下:
#define FF_PROFILE_UNKNOWN -99
#define FF_PROFILE_RESERVED -100
 
#define FF_PROFILE_AAC_MAIN 0
#define FF_PROFILE_AAC_LOW  1
#define FF_PROFILE_AAC_SSR  2
#define FF_PROFILE_AAC_LTP  3
#define FF_PROFILE_AAC_HE   4
#define FF_PROFILE_AAC_HE_V2 28
#define FF_PROFILE_AAC_LD   22
#define FF_PROFILE_AAC_ELD  38
#define FF_PROFILE_MPEG2_AAC_LOW 128
#define FF_PROFILE_MPEG2_AAC_HE  131
 
#define FF_PROFILE_DTS         20
#define FF_PROFILE_DTS_ES      30
#define FF_PROFILE_DTS_96_24   40
#define FF_PROFILE_DTS_HD_HRA  50
#define FF_PROFILE_DTS_HD_MA   60
 
#define FF_PROFILE_MPEG2_422    0
#define FF_PROFILE_MPEG2_HIGH   1
#define FF_PROFILE_MPEG2_SS     2
#define FF_PROFILE_MPEG2_SNR_SCALABLE  3
#define FF_PROFILE_MPEG2_MAIN   4
#define FF_PROFILE_MPEG2_SIMPLE 5
 
#define FF_PROFILE_H264_CONSTRAINED  (1<<9)  // 8+1; constraint_set1_flag
#define FF_PROFILE_H264_INTRA        (1<<11) // 8+3; constraint_set3_flag
 
#define FF_PROFILE_H264_BASELINE             66
#define FF_PROFILE_H264_CONSTRAINED_BASELINE (66|FF_PROFILE_H264_CONSTRAINED)
#define FF_PROFILE_H264_MAIN                 77
#define FF_PROFILE_H264_EXTENDED             88
#define FF_PROFILE_H264_HIGH                 100
#define FF_PROFILE_H264_HIGH_10              110
#define FF_PROFILE_H264_HIGH_10_INTRA        (110|FF_PROFILE_H264_INTRA)
#define FF_PROFILE_H264_HIGH_422             122
#define FF_PROFILE_H264_HIGH_422_INTRA       (122|FF_PROFILE_H264_INTRA)
#define FF_PROFILE_H264_HIGH_444             144
#define FF_PROFILE_H264_HIGH_444_PREDICTIVE  244
#define FF_PROFILE_H264_HIGH_444_INTRA       (244|FF_PROFILE_H264_INTRA)
#define FF_PROFILE_H264_CAVLC_444            44
 
#define FF_PROFILE_VC1_SIMPLE   0
#define FF_PROFILE_VC1_MAIN     1
#define FF_PROFILE_VC1_COMPLEX  2
#define FF_PROFILE_VC1_ADVANCED 3
 
#define FF_PROFILE_MPEG4_SIMPLE                     0
#define FF_PROFILE_MPEG4_SIMPLE_SCALABLE            1
#define FF_PROFILE_MPEG4_CORE                       2
#define FF_PROFILE_MPEG4_MAIN                       3
#define FF_PROFILE_MPEG4_N_BIT                      4
#define FF_PROFILE_MPEG4_SCALABLE_TEXTURE           5
#define FF_PROFILE_MPEG4_SIMPLE_FACE_ANIMATION      6
#define FF_PROFILE_MPEG4_BASIC_ANIMATED_TEXTURE     7
#define FF_PROFILE_MPEG4_HYBRID                     8
#define FF_PROFILE_MPEG4_ADVANCED_REAL_TIME         9
#define FF_PROFILE_MPEG4_CORE_SCALABLE             10
#define FF_PROFILE_MPEG4_ADVANCED_CODING           11
#define FF_PROFILE_MPEG4_ADVANCED_CORE             12
#define FF_PROFILE_MPEG4_ADVANCED_SCALABLE_TEXTURE 13
#define FF_PROFILE_MPEG4_SIMPLE_STUDIO             14
#define FF_PROFILE_MPEG4_ADVANCED_SIMPLE           15
 
#define FF_PROFILE_JPEG2000_CSTREAM_RESTRICTION_0   0
#define FF_PROFILE_JPEG2000_CSTREAM_RESTRICTION_1   1
#define FF_PROFILE_JPEG2000_CSTREAM_NO_RESTRICTION  2
#define FF_PROFILE_JPEG2000_DCINEMA_2K              3
#define FF_PROFILE_JPEG2000_DCINEMA_4K              4 

自此,frame_size,nb_samples,profile之间的关系可以理清楚了,关于mp3的只发现过1152的,如果有其他再记录。 


pcm裸流播放方法
ubuntu 工作环境,
使用命令：sudo apt-get install sox下载播放工具。
完成后,可以使用play命令播放pcm裸流文件了。
首先需要知道对应pcm的文件的采样率,通道数,比特率,
然后使用命令
play -t raw -r 44.1k -e signed-integer -b 16 -c 2 loved.pcm
loved.pcm  这个是文件名
-t 参数后接文件包含数据的格式,我还没高明白raw和pcm这2个音频缩写词包含的具体含义。
-r 数据采样率参数 44.1k
-b 数据比特率参数 8--->128kbps 16--->256kbps 32--->512kbps
-c 通道数        2
下面这个参数没明白,初步猜测是一个数据类型的设置。
-e -e|--encoding ENCODING   Set encoding (ENCODING may be one of 
						 signed-integer(-b 16 or -b 32)一般指定这个值,
                         unsigned-integer, 
                         floating-point, 
                         mu-law, 
                         a-law,
                         ima-adpcm, 
                         ms-adpcm, 
                         gsm-full-rate)

一个多媒体文件包含有多个原始流,例如 movie.mkv这个多媒体文件可能包含下面的流数据
原始流 1 h.264 video
原始流 2 aac audio for Chinese
原始流 3 aac audio for English
原始流 4 Chinese Subtitle
原始流 5 English Subtitle

音频基本概念
1.
通道数
声音在录制时在不同空间位置用不同录音设备采样的声音信号,声音在播放时采用相应个数的扬声器播放。采用多通道的方式是为了丰富声音的现场感。常用的立体声有2个通道,环绕立体声3个通道。数字音频就是有一连串的样本流组成,立体声每次采用要采两次。有点类似视频中的YUV各个分量。
2.
采样率
把模拟信号转换成数字信号在计算机中处理,需要按照一定的采样率采样,样本值就是声音波形中的一个值。音频在播放时按照采样率进行,采样率越高声音的连续性就越好,由于人的听觉器官分辨能力的局限,往往这些数值达到某种程度就可以满足人对“连续”性的需求了。例如22050和44100的采样率就是电台和CD 常用的采样率。类似视频中的帧率。
3.
比特率(bps或kbps)
单位时间所需的空间存储。比特率反应的是视频或者音频一个样本所有的信息量,越大含有的信息量就大。视频中,图像分辨率越大,一帧就越大,实时解码就容易饥渴,传输带宽需要越大,存储空间就越大。音频中描述一个样本就越准确。
4.
帧
视频中帧就是一个图片采样。音频中一帧一般包含多个样本,如AAC格式会包含1024个样本。
5.
样本格式
音频中一个样本存储方式。列举ffmpeg中的样本格式
enum AVSampleFormat {
    AV_SAMPLE_FMT_NONE = -1,
    AV_SAMPLE_FMT_U8,          ///< unsigned 8 bits
    AV_SAMPLE_FMT_S16,         ///< signed 16 bits
    AV_SAMPLE_FMT_S32,         ///< signed 32 bits
    AV_SAMPLE_FMT_FLT,         ///< float
    AV_SAMPLE_FMT_DBL,         ///< double
 
    AV_SAMPLE_FMT_U8P,         ///< unsigned 8 bits, planar
    AV_SAMPLE_FMT_S16P,        ///< signed 16 bits, planar
    AV_SAMPLE_FMT_S32P,        ///< signed 32 bits, planar
    AV_SAMPLE_FMT_FLTP,        ///< float, planar
    AV_SAMPLE_FMT_DBLP,        ///< double, planar
 
    AV_SAMPLE_FMT_NB           ///< Number of sample formats. DO NOT USE if linking dynamically
};
讲一下AV_SAMPLE_FMT_S16和AV_SAMPLE_FMT_S16P格式,AV_SAMPLE_FMT_S16保存一个样本采用有符号16bit交叉存储的方式,AV_SAMPLE_FMT_S16P保存一个样本采用有符号16bit平面存储的方式。举例有两个通道,通道1数据流 c1 c1 c1c1... , 通道2数据流 c2 c2 c2 c2...
平面存储方式：c1 c1 c1c1... c2 c2 c2 c2...
交叉存储方式：c1, c2,c1, c2, c1, c2, ...
AVFrame中平面方式planar每个通道数据存储在data[0], data[1]中,
长度为linesize[0],linesize[1],
交叉方式则所有的数据都存储在data[0],长度为linesize[0]。

mplayer -demuxer rawvideo -rawvideo w=500:h=500 haoke.yuv

ffmpeg采集摄像头视频数据
ffmpeg -f video4linux2 -s 320*240 -r 10 -i /dev/video0 test.asf

ffmpeg2.0最新的解码出来的数据是 sample_fmts  = AV_SAMPLE_FMT_FLTP
android需要的音频格式：sample_fmts (AV_SAMPLE_FMT_S8, AV_SAMPLE_FMT_S16) 

问题:




通过av_get_channel_layout_nb_channels()和av_get_default_channel_layout()这些函数可以得到channels和channellayout的转换。
libavutil中的audioconvert.c定义channellayout和channels的相关map：
channel_layout_map[]
	{ "mono",        1,  AV_CH_LAYOUT_MONO },
    { "stereo",      2,  AV_CH_LAYOUT_STEREO },
    { "2.1",         3,  AV_CH_LAYOUT_2POINT1 },
    { "3.0",         3,  AV_CH_LAYOUT_SURROUND },
    { "3.0(back)",   3,  AV_CH_LAYOUT_2_1 },
    { "4.0",         4,  AV_CH_LAYOUT_4POINT0 },
    { "quad",        4,  AV_CH_LAYOUT_QUAD },
    { "quad(side)",  4,  AV_CH_LAYOUT_2_2 },
    { "3.1",         4,  AV_CH_LAYOUT_3POINT1 },
    { "5.0",         5,  AV_CH_LAYOUT_5POINT0_BACK },
    { "5.0(side)",   5,  AV_CH_LAYOUT_5POINT0 },
    { "4.1",         5,  AV_CH_LAYOUT_4POINT1 },
    { "5.1",         6,  AV_CH_LAYOUT_5POINT1_BACK },
    { "5.1(side)",   6,  AV_CH_LAYOUT_5POINT1 },
    { "6.0",         6,  AV_CH_LAYOUT_6POINT0 },
    { "6.0(front)",  6,  AV_CH_LAYOUT_6POINT0_FRONT },
    { "hexagonal",   6,  AV_CH_LAYOUT_HEXAGONAL },
    { "6.1",         7,  AV_CH_LAYOUT_6POINT1 },
    { "6.1",         7,  AV_CH_LAYOUT_6POINT1_BACK },
    { "6.1(front)",  7,  AV_CH_LAYOUT_6POINT1_FRONT },
    { "7.0",         7,  AV_CH_LAYOUT_7POINT0 },
    { "7.0(front)",  7,  AV_CH_LAYOUT_7POINT0_FRONT },
    { "7.1",         8,  AV_CH_LAYOUT_7POINT1 },
    { "7.1(wide)",   8,  AV_CH_LAYOUT_7POINT1_WIDE },
    { "octagonal",   8,  AV_CH_LAYOUT_OCTAGONAL },
    { "downmix",     2,  AV_CH_LAYOUT_STEREO_DOWNMIX, },


PCM音频数据是如何存储的
单声道音频,采样数据按时间的先后顺序依次存入
双声道音频,按照LRLRLR的方式存储,存储的时候还和机器的大小端有关

8位单声道:0声道,0声道......
8位双声道:0声道(左),1声道(右),0声道(左),1声道(右)......
16位单声道:0声道(低字节),0声道(高字节),0声道(低字节),0声道(高字节)......
16位双声道:0声道(左,低字节),0声道(左,高字节),1声道(右,低字节),1声道(右,高字节)......

PCM音频数据中常用的专业术语
一般我们描述PCM音频数据的参数的时候有如下描述方式:
44100Hz 16bit stereo:每秒钟有44100次采样,采样数据用16位(2字节)记录,双声道(立体声)；
22050Hz 8bit  mono:每秒钟有22050次采样,采样数据用8位(1字节)记录,单声道；
 44100Hz指的是采样率,它的意思是每秒取样44100次。采样率越大,存储数字音频所占的空间就越大。
16bit指的是采样精度,意思是原始模拟信号被采样后,每一个采样点在计算机中用16位(两个字节)来表示。采样精度越高越能精细地表示模拟信号的差异。
一般来说PCM数据中的波形幅值越大,代表音量越大。

对于ffmpeg来说,音频数据会保存在AVFrame中extended_data数组中,如果是打包模式(packed),就只用extended_data[0],如果是planar模式,则每个channel分别保存在extended_data[i]中。对于音频,只有linesize[0]有效,打包模式保存整个音频帧的buff大小,planar模式保存每个channel的buff大小。
short *sample_buffer_L = pFrame->extended_data[0];//存放着左声道的数据
short *sample_buffer_R = pFrame->extended_data[1];//存放着右声道的数据
两者都是16bit,而裸的PCM文件里的数据是按照 LRLRLRLR  这样存储的,所以我们需要按照这种格式存储16bit的数据：
//Left channel
data[i] = (char)(sample_buffer_L[j] & 0xff);//左声道低8位
data[i+1] = (char)((sample_buffer_L[j]>>8) & 0xff);;//左声道高8位
//Right channel
data[i+2] = (char)(sample_buffer_R[j] & 0xff);//右声道低8位
data[i+3] = (char)((sample_buffer_R[j]>>8) & 0xff);;//右声道高8位

问题:
哪些封装格式的视频需要写文件头,哪些不需要.
音频封装格式解码为pcm时,是不是只能改变采样格式,其他的如采样率,比特率,声道数都不能改变?

./configure \
--enable-libx264 \
--enable-gpl \
--enable-decoder=h264 \
--enable-encoder=libx264 \
--enable-shared \
--enable-static \
--enable-nonfree \
--enable-libfdk-aac \
--disable-yasm \
--prefix=/root/mydev/tools/ffmpeg

--prefix=/root/mydev/tools/ffmpeg --enable-libmp3lame --enable-libx264 --enable-gpl

//use it
./configure \
--enable-gpl \
--enable-openssl \
--enable-decoder=h264 \
--enable-decoder=vpx \
--enable-encoder=libx264 \
--enable-encoder=libvpx \
--enable-nonfree \
--enable-libmp3lame \
--enable-libfdk-aac \
--enable-libvpx \
--enable-libx264 \
--disable-yasm \
--prefix=/root/mydev/tools/ffmpeg

--enable-decoder=h265 \
--enable-encoder=libx265 \
--enable-libx265 \




--disable-static \
--enable-shared \
--enable-gpl \
--enable-version3 \
--disable-w32threads \
--enable-avisynth \
--enable-bzlib \
--enable-fontconfig \
--enable-frei0r \
--enable-gnutls \
--enable-iconv \
--enable-libass \
--enable-libbluray \
--enable-libcaca \
--enable-libfreetype \
--enable-libgsm \
--enable-libilbc \
--enable-libmodplug \
--enable-libmp3lame \
--enable-libopencore-amrnb \
--enable-libopencore-amrwb \
--enable-libopenjpeg \
--enable-libopus \
--enable-librtmp \
--enable-libschroedinger \
--enable-libsoxr \
--enable-libspeex \
--enable-libtheora \
--enable-libtwolame \
--enable-libvidstab \
--enable-libvo-aacenc \
--enable-libvo-amrwbenc \
--enable-libvorbis \
--enable-libvpx \
--enable-libwavpack \
--enable-libx264 \
--enable-libx265 \
--enable-libxavs \
--enable-libxvid \
--enable-decklink \
--enable-zlib \



//音频输出参数
//声道布局
uint64_t out_channel_layout = AV_CH_LAYOUT_STEREO;
//采样格式
AVSampleFormat out_sample_fmt = AV_SAMPLE_FMT_S16;
//采样率
int out_sample_rate = 44100;

//根据声道格式返回声道个数
int out_nb_channels = av_get_channel_layout_nb_channels(out_channel_layout);
//采样个数nb_samples: AAC-1024 MP3-1152 
int out_nb_samples = pCodecCtx->frame_size;
//写到文件中去时要用到,就是写入多少个字节
int out_buffer_size = av_samples_get_buffer_size(NULL, 
								out_nb_channels, out_nb_samples, out_sample_fmt, 1);

// 1 second of 48khz 32bit(4Byte) audio
#define MAX_AUDIO_FRAME_SIZE 192000
uint8_t *buffer = (uint8_t *)av_malloc(MAX_AUDIO_FRAME_SIZE);

audio_convert_ctx = swr_alloc();
if (audio_convert_ctx == NULL)
{
	printf("Could not allocate SwrContext\n");
	return -1;
}
swr_alloc_set_opts(audio_convert_ctx, 
					out_channel_layout, 
					out_sample_fmt,
					out_sample_rate, 
					pCodecCtx->channel_layout, 
					pCodecCtx->sample_fmt, 
					pCodecCtx->sample_rate, 
					0, NULL);
swr_init(audio_convert_ctx);

//原始数据放在AVPacket中,解码后的数据放在AVFrame中
if (avcodec_decode_audio4(pCodecCtx, pAudioFrame, &got_picture, &packet) < 0)
{
	printf("Error in decoding audio frame.\n");
	return -1;
}
//为什么要做这步操作:
//解码得到的数据类型为float 4bit,而播放器播放的格式一般为S16(signed 16bit),就需要对解码得到的数据进行转换
//转换前的数据放在AVFrame中,转换后的数据放在uint8_t中
swr_convert(audio_convert_ctx, &buffer, MAX_AUDIO_FRAME_SIZE, 
			(const uint8_t **)pAudioFrame->data, pAudioFrame->nb_samples);
//一次解码得到的数据大小为out_buffer_size
fwrite(buffer, 1, out_buffer_size, fp_pcm);

处理音频时,需要用到的处理数据的变量有
AVPacket avcodec_decode_audio4(...)
AVFrame  swr_convert(...)
uint8_t  fwrite(...)
有关的概念有:声道布局,声道数,采样个数,采样率,比特率


有些格式的视频不符合标准,获得的pCodecCtx->frame_size为0
雷老师,我使用wmv格式的视频进行测试,结果不能进行得到正确的pcm文件,文件大小始终为0,发现是out_nb_samples = pCodecCtx->frame_size发生错误,其中pCodecCtx->frame_size为0,导致av_samples_get_buffer_size算出的大小是负数。问过有经验组长,他告诉我有些格式的视频不符合标准,不能从文件头中获取到信息,要在读入一帧后获取,也就是ret = avcodec_decode_audio4( pCodecCtx, pFrame,&got_picture, packet);之后,从pFrame中获取信息。
发现错误,重新修改程序,就能得到数据了。


Stream #0:0: Video: h264 (High), yuv420p(progressive), 480x272, 25 fps, 25 tbr, 1200k tbn, 50 tbc
Stream #0:0: Video: hevc (Main), yuv420p(tv), 480x272, 25 fps, 25 tbr, 1200k tbn, 25 tbc
Stream #0:0: Video: mpeg2video (Main), yuv420p(tv, progressive), 480x272 [SAR 1:1 DAR 30:17], 25 fps, 25 tbr, 1200k tbn, 50 tbc


struct FormatCtx {
  inline FormatCtx(const char* filename)
  : ctx_(avformat_alloc_context()) {
    av_init_packet(&p);
    if (avformat_open_input(&ctx_, filename, 0, 0) < 0)
      abort();
    if (avformat_find_stream_info(ctx_, 0) < 0)
      abort();
  }

  inline ~FormatCtx() {
    av_free_packet(&p);
  }

  inline bool read() {
    return av_read_frame(ctx_, &p) >= 0;
  }

  AVFormatContext* ctx_;
  AVPacket p;
} formatCtx_;


1. 采样率
    采样设备每秒抽取样本的次数
2. 音频格式及量化精度(位宽)
    每种音频格式有不同的量化精度(位宽),位数越多,表示值就越精确,声音表现自然就越精准。
3. 分片(plane)和打包(packed)
    以双声道为例,带P(plane)的数据格式在存储时,其左声道和右声道的数据是分开存储的,左声道的数据存储在data[0],右声道的数据存储在data[1],每个声道的所占用的字节数为linesize[0]和linesize[1]；
    不带P(packed)的音频数据在存储时,是按照LRLRLR...的格式交替存储在data[0]中,linesize[0]表示总的数据量。
4. 声道分布(channel_layout)
    声道分布在FFmpeg\libavutil\channel_layout.h中有定义,一般来说用的比较多的是AV_CH_LAYOUT_STEREO(双声道)和AV_CH_LAYOUT_SURROUND(三声道)
5. 音频帧的数据量计算
    一帧音频的数据量=channel数 * nb_samples样本数 * 每个样本占用的字节数
    如果该音频帧是FLTP格式的PCM数据,包含1024个样本,双声道,那么该音频帧包含的音频数据量是2*1024*4=8192字节

ffmpeg重采样中swr_convert和swr_get_out_samples的用法
在做mux的时候关于重采样可以用fifo,或者audiofifo做缓存处理,
当做demux的时候关于重采样就可以用到上面的swr_convert和swr_get_out_samples做配合处理。
如果传入的nb_samles大于了传出的nb_samplse则SwrContext中会有缓存,会导致内存一直暴涨,解决方法,可以看如下代码：
没有缓存的重采样这么处理：
ret = swr_convert(swrcontext, pOutputFrame->data,pOutputFrame->nb_samples,
		(const uint8_t**)pInputFrame->data,pInputFrame->nb_samples);
有缓存的代码这么处理： 
//如果还有缓存在swrcontext中,第二个参数要填写0才能获取到,缓存数据
int fifo_size = swr_get_out_samples(swrcontext,0);
if ( fifo_size >= pOutputFrame->nb_samples)
{
	ret = swr_convert(swrcontext, pOutputFrame->data,pOutputFrame->nb_samples,
	NULL,0);
}
即如果有缓存则先判断是否有缓存在里面,如果有则传入数据为空取出缓存。


庖丁解牛,彻底掌握c++类型转换
#include using namespace std;class Base {public:int objb;Base (int a) : objb(a){cout << "Base .... " << endl;}Base (const Base &other):objb(other.objb){cout << "Base copy ...." <<endl;}~Base (){cout << "~Base .... " << endl;}};class Derived :public Base {public:int objd;Derived (int b, int d) :Base(b),objd(d) {cout << "Derived .... " << endl;}~Derived () {cout << "~Derived .... " << endl;}};class A {};class B {public:B(const Base& base):bb(base.objb){cout << "B .... " << endl;}~B () {cout << "~B .... " << endl;}int cc;int bb;};class virtualA {public:virtual void funA(){} // ~virtualA(){}};class virtualB : public virtualA{};class virtualC {public:virtual void funC (){}};int main(){int a = 10;double d1 = static_cast(a);//double dre = static_cast(a);//int 和double之间没有父子类关系,所以 int不能转换成double&,而如果是父子类则 可以把 Base对像 转成Derived&double d2 = reinterpret_cast(a);//原封不动拷贝内存,把int转换成 double&,然后用double的性质来解释 int//double d3 = dynamic_cast(a);double d4 = a;cout << d1 << endl;cout << d2 << endl;cout << d4 << endl;cout <<"------------------------------------------------" << endl;double d = 10000000000.1;cout << "d = " << d << endl;int b1 = static_cast(d);cout << b1 << endl;int b2 = reinterpret_cast(d); cout << b2 << endl;//int b3 = dynamic_cast (d);int b4 = d;cout << b4 << endl;int b5 = (int)d;cout << b5 << endl;cout << "------------------------------------------------" << endl;Base bObj(10);Derived dObj(10,11);bObj = dObj;//调用拷贝构造函数cout << bObj.objb << endl;Base bObj2 = static_cast(dObj);//分成两步,1父类引用指向子类对象;2将该引用的对象拷贝到目标对象,static_cast只是做了第一步cout << bObj2.objb << endl;Base bObj3 = static_cast(dObj);//直接调用拷贝构造函数cout << bObj3.objb << endl;cout << "following is reinterpret_cast " << endl;Base bObj4 = reinterpret_cast (dObj);//也分成两步,1父类引用指向子类对象 2将该引用的对象拷贝到目标对象cout << bObj4.objb << endl;cout << "reinterpret_cast end" << endl;//Base bObj5 = dynamic_cast (dObj);//Derived dObj2 = bObj;//cout << dObj2.objb <<" "<< dObj2.objd << endl;Derived dObj3 = static_cast(bObj);//类型中没有 & 编译出错,因为没有对应的构造函数供调用,这里强制下行转换cout << dObj3.objb <<" "<< dObj3.objd << endl;Derived dObj4 = reinterpret_cast (bObj);//类型中没有&编译不过cout << dObj4.objb <<" "<< dObj4.objd << endl;// A aObj = static_cast (bObj); //static_cast 调用了目标类型的转换构造函数,所以编译时检查类型安全// A aObj = static_cast (bObj);//转换成引用类型没有调用转换构造函数,对象和引用所代表的类不存在父子关系B bb = static_cast (bObj);//调用转换构造函数 转换成功// B bb1 = static_cast (bObj);//编译期类型检查,所以无法成功B bb2 = reinterpret_cast (bObj);//reterinpret_cast不做任何检测,转换成功cout << "bb2.bb= " << bb2.bb << endl;//reinterpret_cast 只是把内存原模原样的复制过来cout << "&bb2=" << &bb2 << endl;cout << "&bObj=" << &bObj << endl;cout << "-------------------------------------------------" << endl;Base base1(20);Derived derived1(20,21);Base *pBase1 = &derived1;//Derived *pDerived = &base1;Derived *pDerived1 = static_cast (&base1);//父子类之间的指针转换 上行安全,下行虽不安全也可以转cout << pDerived1->objb << endl;cout << pDerived1->objd << endl;Derived* pDerived2 = reinterpret_cast (&base1);//你想干啥就干啥,安全自己负责cout << pDerived2->objb << endl;cout << pDerived2->objd << endl;//Derived* pDerived3 = dynamic_cast (&base1);//dynamic_cast 只能转换指针和引用类型,且必须polymorphic(多态,也就是虚函数)cout << "--------------------------------" << endl;virtualA va1;virtualB vb1;virtualA* pva = NULL;virtualB* pvb = NULL;pva = &va1;pvb = dynamic_cast(&va1);//继承了有虚函数的父类可以通过编译,但能否转换成功需要在运行时才知道,运行时检查不成功则返回空指针;cout << "pvb="<<pvb<<endl; //转换失败因为不安全pva = dynamic_cast (&vb1);cout << "pva=" <<pva<< endl;//转换成功 ,因为类型安全virtualC vc1;virtualC* pvc = NULL;pvc = dynamic_cast(&va1);//通过编译但是 类型转换不安全,所以运行时类型转换失败cout << "pvc = " << pvc << endl;cout << "-----------------------------------" << endl;//vb1 = dynamic_cast(va1);//通过编译,但是运行时抛出bad_alloc异常,因为无法转换va1 = dynamic_cast (vb1);//通过编译且转换正常cout << "-------析构开始------"<< endl;}



音频封装遇到的问题：
 Malformed AAC bitstream detected: use the audio bitstream filter 'aac_adtstoasc' to fix it ('-bsf:a aac_adtstoasc' option with ffmpeg)
解决：
在音频数据处理之前,加上这句即可：
AVBitStreamFilterContext* aacbsfc = av_bitstream_filter_init("aac_adtstoasc");
原因是FIX:AAC in some container format (FLV, MP4, MKV etc.) need "aac_adtstoasc" bitstream filter (BSF)

http://www.cnblogs.com/azraelly/archive/2013/01/01/2841269.html  图文详解YUV420数据格式
YUV格式有两大类：planar(平面)和packed(交错)。
(比如一帧的数据存储方式)
对于planar的YUV格式,YYYYYUUUUUVVVVV。
对于packed的YUV格式,YUVYUVYUVYUVYUV。

YUV码流的存储格式其实与其采样的方式密切相关,主流的采样方式有三种:
YUV4:4:4
YUV4:2:2
YUV4:2:0

提取每个像素的YUV分量
YUV 4:4:4采样,每一个Y对应一组UV分量。
YUV 4:2:2采样,每两个Y共用一组UV分量。 
YUV 4:2:0采样,每四个Y共用一组UV分量。

存储方式:
1.
YUYV格式(属于YUV422)

2.
UYVY格式(属于YUV422)

3.
YUV422P格式(属于YUV422)

4.
YV12,YU12格式(属于YUV420)

5.
NV12,NV21格式(属于YUV420)

YUV420P,Y,U,V三个分量都是平面格式,有I420和YV12两种格式。
I420格式和YV12格式的不同之处在于U平面和V平面的位置不同。
在I420格式中,U平面紧跟在Y平面之后,然后才是V平面(即:YUV);但YV12则是相反(即:YVU)。
YUV420SP, Y分量平面格式,UV打包格式, 即NV12。 
NV12与NV21类似,U 和 V 交错排列,不同在于UV顺序。
I420: YYYYYYYY UU VV    =>YUV420P
YV12: YYYYYYYY VV UU    =>YUV420P
NV12: YYYYYYYY UV UV    =>YUV420SP
NV21: YYYYYYYY VU VU    =>YUV420SP

I420和YV12的区别        
一般来说,直接采集到的视频数据是RGB24格式,
RGB24一帧的大小size＝width×heigth×3 Bit,
RGB32一帧的大小size＝width×heigth×4 Bit,
如果是I420(即YUV标准格式4：2：0)的数据量是 size＝width×heigth×1.5 Bit。       在采集到RGB24数据后,需要对这个格式的数据进行第一次压缩。即将图像的颜色空间由RGB2YUV。因为,X264在进行编码的时候需要标准的YUV(4：2：0)。但是这里需要注意的是,虽然YV12也是(4：2：0),但是YV12和I420的却是不同的,在存储空间上面有些区别。如下： YV12 ： 亮度(行×列) ＋ U(行×列/4) + V(行×列/4)

I420 ： 亮度(行×列) ＋ V(行×列/4) + U(行×列/4)


AVFifoBuffer * m_fifo = NULL;
 
 
 
SwrContext * init_pcm_resample(AVFrame *in_frame, AVFrame *out_frame)
{
	SwrContext * swr_ctx = NULL;
	swr_ctx = swr_alloc();
	if (!swr_ctx)
	{
		printf("swr_alloc error \n");
		return NULL;
	}
	AVCodecContext * audio_dec_ctx = icodec->streams[audio_stream_idx]->codec;
	AVSampleFormat sample_fmt;
	sample_fmt = (AVSampleFormat)m_dwBitsPerSample; //样本
	if (audio_dec_ctx->channel_layout == 0)
	{
		audio_dec_ctx->channel_layout = av_get_default_channel_layout(icodec->streams[audio_stream_idx]->codec->channels);
	}
	/* set options */
	av_opt_set_int(swr_ctx, "in_channel_layout",    audio_dec_ctx->channel_layout, 0);
	av_opt_set_int(swr_ctx, "in_sample_rate",       audio_dec_ctx->sample_rate, 0);
	av_opt_set_sample_fmt(swr_ctx, "in_sample_fmt", audio_dec_ctx->sample_fmt, 0);
 
	av_opt_set_int(swr_ctx, "out_channel_layout",    audio_dec_ctx->channel_layout, 0);
	av_opt_set_int(swr_ctx, "out_sample_rate",       audio_dec_ctx->sample_rate, 0);
	av_opt_set_sample_fmt(swr_ctx, "out_sample_fmt", sample_fmt, 0);
	swr_init(swr_ctx);
 
	int64_t src_nb_samples = in_frame->nb_samples;
	out_frame->nb_samples = av_rescale_rnd(swr_get_delay(swr_ctx,oaudio_st->codec->sample_rate) + src_nb_samples,
		oaudio_st->codec->sample_rate, oaudio_st->codec->sample_rate, AV_ROUND_UP);
 
	int ret = av_samples_alloc(out_frame->data, &out_frame->linesize[0], 
		icodec->streams[audio_stream_idx]->codec->channels, out_frame->nb_samples,oaudio_st->codec->sample_fmt,1);
	if (ret < 0)
	{
		return NULL;
	}
 
	//pcm分包初始化
	int buffersize = av_samples_get_buffer_size(NULL, oaudio_st->codec->channels,
		2048, oaudio_st->codec->sample_fmt, 1);
	m_fifo = av_fifo_alloc(buffersize);
	return swr_ctx;
}
 
int preform_pcm_resample(SwrContext * pSwrCtx,AVFrame *in_frame, AVFrame *out_frame)
{
	int ret = 0;
	if (pSwrCtx != NULL) 
	{
		ret = swr_convert(pSwrCtx, out_frame->data, out_frame->nb_samples, 
			(const uint8_t**)in_frame->data, in_frame->nb_samples);
		if (ret < 0)
		{
			return -1;
		}
		//修改分包内存
		int buffersize = av_samples_get_buffer_size(&out_frame->linesize[0], oaudio_st->codec->channels,
			ret, oaudio_st->codec->sample_fmt, 1);
		int sss = av_fifo_size(m_fifo);
		sss = av_fifo_realloc2(m_fifo, av_fifo_size(m_fifo) + out_frame->linesize[0]);
		sss = av_fifo_size(m_fifo);
		av_fifo_generic_write(m_fifo, out_frame->data[0], out_frame->linesize[0], NULL);
 
		out_frame->pkt_pts = in_frame->pkt_pts;
		out_frame->pkt_dts = in_frame->pkt_dts;
		//有时pkt_pts和pkt_dts不同,并且pkt_pts是编码前的dts,这里要给avframe传入pkt_dts而不能用pkt_pts
		//out_frame->pts = out_frame->pkt_pts;
		out_frame->pts = in_frame->pkt_dts;
	}
	return 0;
}
void uinit_pcm_resample(AVFrame * poutframe,SwrContext * swr_ctx)
{
	if (poutframe)
	{
		avcodec_free_frame(&poutframe);
		poutframe = NULL;
	}
	if (swr_ctx)
	{
		swr_free(&swr_ctx);
		swr_ctx = NULL;
	}
	//析构pcm分包结构
	if(m_fifo)
	{
		av_fifo_free(m_fifo);
		m_fifo = NULL;
	}
}
int perform_code(int stream_type,AVFrame * picture)
{
	AVCodecContext *cctext = NULL;
	AVPacket pkt_t;
	av_init_packet(&pkt_t);
	pkt_t.data = NULL; // packet data will be allocated by the encoder
	pkt_t.size = 0;
	int frameFinished = 0 ;
 
	if (stream_type == AUDIO_ID)
	{
		cctext = oaudio_st->codec;
		//如果进和出的的声道,样本,采样率不同,需要重采样
		if(icodec->streams[audio_stream_idx]->codec->sample_fmt != (AVSampleFormat)m_dwBitsPerSample ||
			icodec->streams[audio_stream_idx]->codec->channels != m_dwChannelCount ||
			icodec->streams[audio_stream_idx]->codec->sample_rate != m_dwFrequency)
		{
			int64_t pts_t = picture->pts;
			int duration_t = (double)cctext->frame_size * (icodec->streams[audio_stream_idx]->time_base.den /icodec->streams[audio_stream_idx]->time_base.num)/ 
				icodec->streams[audio_stream_idx]->codec->sample_rate;
 
			int frame_bytes = cctext->frame_size * av_get_bytes_per_sample(cctext->sample_fmt)* cctext->channels;
			AVFrame * pFrameResample = avcodec_alloc_frame();
			uint8_t * readbuff = new uint8_t[frame_bytes];
 
			if(av_sample_fmt_is_planar(cctext->sample_fmt))
			{
				frame_bytes /= cctext->channels;
			}
 
			while (av_fifo_size(m_fifo) >= frame_bytes) //取出写入的未读的包
			{
				pFrameResample->nb_samples = cctext->frame_size;
				av_fifo_generic_read(m_fifo, readbuff, frame_bytes, NULL);
 
				//这里一定要考虑音频分片的问题
				//如果是分片的avcodec_fill_audio_frame传入的buf是单声道的,但是buf_size 是两个声道加一起的数据量
				//如果不是分片的avcodec_fill_audio_frame传入的buf是双声道的,buf_size 是两个声道加一起的数据量
				if(av_sample_fmt_is_planar(cctext->sample_fmt))
				{
					avcodec_fill_audio_frame(pFrameResample,cctext->channels,cctext->sample_fmt,readbuff,frame_bytes * cctext->channels,1);
				}
				else
				{					
					avcodec_fill_audio_frame(pFrameResample,cctext->channels,cctext->sample_fmt,readbuff,frame_bytes,0);
				}
 
				if(m_is_first_audio_pts == 0)
				{
					m_first_audio_pts = pts_t;
					m_is_first_audio_pts = 1;
				}
				pFrameResample->pts = m_first_audio_pts;
				m_first_audio_pts += duration_t;
 
 
				pFrameResample->pts = av_rescale_q_rnd(pFrameResample->pts, icodec->streams[audio_stream_idx]->codec->time_base, oaudio_st->codec->time_base, AV_ROUND_NEAR_INF);
				nRet = avcodec_encode_audio2(cctext,&pkt_t,pFrameResample,&frameFinished);
				if (nRet>=0 && frameFinished)
				{
					write_frame(ocodec,AUDIO_ID,pkt_t);
					av_free_packet(&pkt_t);
				}
			}
			if (readbuff)
			{
				delete []readbuff;
			}
			if (pFrameResample)
			{
				av_free(pFrameResample);
				pFrameResample = NULL;
			}
		}
		else
		{
			nRet = avcodec_encode_audio2(cctext,&pkt_t,picture,&frameFinished);
			if (nRet>=0 && frameFinished)
			{
				write_frame(ocodec,AUDIO_ID,pkt_t);
				av_free_packet(&pkt_t);
			}
		}
	}
	else if (stream_type == VIDEO_ID)
	{
		cctext = ovideo_st->codec;
		if(icodec->streams[video_stream_idx]->codec->ticks_per_frame != 1)
		{
			AVRational time_base_video_t;
			time_base_video_t.num = icodec->streams[video_stream_idx]->codec->time_base.num;
			time_base_video_t.den = icodec->streams[video_stream_idx]->codec->time_base.den /icodec->streams[video_stream_idx]->codec->ticks_per_frame;
			picture->pts = av_rescale_q_rnd(picture->pts, time_base_video_t, ovideo_st->codec->time_base, AV_ROUND_NEAR_INF);
		}
		else
		{
			picture->pts = av_rescale_q_rnd(picture->pts, icodec->streams[video_stream_idx]->codec->time_base, ovideo_st->codec->time_base, AV_ROUND_NEAR_INF);
		}
		avcodec_encode_video2(cctext,&pkt_t,picture,&frameFinished);
		picture->pts++;
		if (frameFinished)
		{
			write_frame(ocodec,VIDEO_ID,pkt_t);
			av_free_packet(&pkt_t);
		}
	}
	return 1;
}


ffmpeg -h encoder=libx264
ffmpeg version N-91447-g0bd48ab Copyright (c) 2000-2018 the FFmpeg developers
  built with gcc 4.8 (Ubuntu 4.8.5-4ubuntu2)
  configuration: --enable-gpl --enable-openssl --enable-decoder=h264 --enable-decoder=vpx --enable-encoder=libx264 --enable-encoder=libvpx --enable-nonfree --enable-libmp3lame --enable-libfdk-aac --enable-libvpx --enable-libx264 --disable-yasm --prefix=/root/mydev/tools/ffmpeg





















FFmpeg从入门到精通
最强Android书 架构大剖析
高级Android开发强化实战
C++并发编程实战
Python神经网络编程
Qt 5.9 C++开发指南 
OpenCV算法精解：基于Python与C++
Android组件化架构
Linux网络服务与Shell脚本攻略
跟老男孩学Linux运维：核心系统命令实战
机器人Python极客编程入门与实战



MediaCodec.INFO_TRY_AGAIN_LATER = -1
MediaCodec.INFO_OUTPUT_FORMAT_CHANGED = -2
MediaCodec.INFO_OUTPUT_BUFFERS_CHANGED = -3
1.
MediaCodec.INFO_TRY_AGAIN_LATER
int index = mediaEncode.dequeueOutputBuffer(MediaCodec.BufferInfo info, long timeoutUs);
如果调用上面的代码时,timeoutUs为一个正整数(表示超时)时,
在运行过程中发生超时情况,那么得到的index就为-1.
发生这种情况时可以这样处理:
退出本次循环,然后从头开始循环.
2.
MediaCodec.INFO_OUTPUT_FORMAT_CHANGED
输出格式发生了变化,随后的数据将按照新格式。
然后再调用上面代码得到的index就为-2.
发生这种情况时可以这样处理:
mMediaFormat = mVideoEncoder.getOutputFormat();
outputVideoTrack = mMediaMuxer.addTrack(mMediaFormat);
重新调整一下视频或者音频的TrackIndex,然后continue.
3.
MediaCodec.INFO_OUTPUT_BUFFERS_CHANGED
发生这种情况时可以这样处理:
outputBuffers = mVideoEncoder.getOutputBuffers();
重新调整一下房间,然后continue.

/***
这样去理解:
mediaDecode.configure(format, null, null, 0);
mediaDecode.start();
上面代码调用之后,使用
mediaDecode.getInputBuffers();
可以得到这样一个ByteBuffer数组,
每一个数组元素想像成一个小房间,
房间之间都是连续的,每个小房间里存放数据.
*/
// MediaCodec在此ByteBuffer[]中获取输入数据
ByteBuffer[] decodeInputBuffers = mediaDecode.getInputBuffers();
// MediaCodec将解码后的数据放到此ByteBuffer[]中 我们可以直接在这里面得到PCM数据
ByteBuffer[] decodeOutputBuffers = mediaDecode.getOutputBuffers();
// 用于描述解码得到的byte[]数据的相关信息
decodeBufferInfo = new MediaCodec.BufferInfo();

int inputIndex = mediaDecode.dequeueInputBuffer(-1);
把inputIndex想像成房间号,如果这个房间号小于0,就表示
暂时没有空闲的房间可以用来存放需要解码的数据;
如果大于0,就表示有空闲的房间可以用来存放需要解码的数据.
然后通过
ByteBuffer inputBuffer = decodeInputBuffers[inputIndex];
inputBuffer.clear();
得到对应房间号的房间,inputBuffer就是用来存放需要解码的数据,
拿到这个房间后先打扫一下,然后再放数据才是好的做法.
需要解码的数据是这样来的,代码为
int sampleSize = mediaExtractor.readSampleData(inputBuffer, 0);
调用之后,需要解码的数据就被放入到了inputBuffer中了.
如果sampleSize小于0,表示需要解码的数据没了,不用再解码了,退出好了.
如果sampleSize大于0,那么调用这样的代码:
// 通知MediaDecode解码刚刚传入的数据
mediaDecode.queueInputBuffer(inputIndex, 0, sampleSize, 0, 0);
这句代码的意思可以想像成,通知一下mediaDecode,我已经把需要
解码的数据给你了,你可以进行解码的工作了.
然后调用下面代码进行下一次的工作.
// MediaExtractor移动到下一取样处
mediaExtractor.advance();




Android音视频开发(一): 通过三种方式绘制图片
<uses-permission android:name="android.permission.READ_EXTERNAL_STORAGE"/>
在Android平台绘制一张图片,使用至少3种不同的API,ImageView,SurfaceView,自定义View。
1. ImageView绘制图片
Bitmap bitmap = BitmapFactory.decodeFile(Environment.getExternalStorageDirectory().getPath() + File.separator + "11.jpg");
imageView.setImageBitmap(bitmap);
2. SurfaceView绘制图片
SurfaceView surfaceView = (SurfaceView) findViewById(R.id.surface);
surfaceView.getHolder().addCallback(new SurfaceHolder.Callback() {
    @Override
    public void surfaceCreated(SurfaceHolder surfaceHolder) {
        if (surfaceHolder == null) {
            return;
        }

        Paint paint = new Paint();
        paint.setAntiAlias(true);
        paint.setStyle(Paint.Style.STROKE);

        // 获取bitmap
        Bitmap bitmap = BitmapFactory.decodeFile(Environment.getExternalStorageDirectory().getPath() + File.separator + "11.jpg"); 
        // 先锁定当前surfaceView的画布
        Canvas canvas = surfaceHolder.lockCanvas();  
        //执行绘制操作
        canvas.drawBitmap(bitmap, 0, 0, paint); 
        // 解除锁定并显示在界面上
        surfaceHolder.unlockCanvasAndPost(canvas); 
    }

    @Override
    public void surfaceChanged(SurfaceHolder surfaceHolder, int i, int i1, int i2) {

    }

    @Override
    public void surfaceDestroyed(SurfaceHolder surfaceHolder) {

    }
});
3. 自定义View绘制图片
public class CustomView extends View {

    Paint paint = new Paint();
    Bitmap bitmap;

    public CustomView(Context context) {
        super(context);
        paint.setAntiAlias(true);
        paint.setStyle(Paint.Style.STROKE);
        // 获取bitmap
        bitmap = BitmapFactory.decodeFile(Environment.getExternalStorageDirectory().getPath() + File.separator + "11.jpg");  
    }

    @Override
    protected void onDraw(Canvas canvas) {
        super.onDraw(canvas);

        // 不建议在onDraw做任何分配内存的操作
        if (bitmap != null) {
            canvas.drawBitmap(bitmap, 0, 0, paint);
        }
    }

}

Android音视频开发(二)：使用AudioRecord采集音频PCM并保存到文件
<uses-permission android:name="android.permission.RECORD_AUDIO"/>
https://github.com/renhui/AndroidRecorder.git
https://github.com/renhui/OpenGLVideoRecord.git
https://github.com/renhui/AudioDemo.git

使用AudioRecord实现录音,并生成wav
2.1  创建一个AudioRecord对象
首先要声明一些全局的变量参数：
// 声明 AudioRecord 对象
private AudioRecord audioRecord = null;  
// 声明recoordBufffer的大小字段
private int recordBufSize = 0; 
获取buffer的大小并创建AudioRecord：
public void createAudioRecord() {
	// audioRecord能接受的最小的buffer大小
　　	recordBufSize = AudioRecord.getMinBufferSize(
					frequency, channelConfiguration, EncodingBitRate);  
   	audioRecord = new AudioRecord(
   					MediaRecorder.AudioSource.MIC, frequency, 
   					channelConfiguration, EncodingBitRate, recordBufSize);
}
2.2  初始化一个buffer
byte data[] = new byte[recordBufSize]; 
2.3  开始录音
audioRecord.startRecording();
isRecording = true;
2.4  创建一个数据流,
		一边从AudioRecord中读取声音数据到初始化的buffer,
		一边将buffer中数据导入数据流。
FileOutputStream os = null;
try {
    os = new FileOutputStream(filename);
} catch (FileNotFoundException e) {
    e.printStackTrace();
}
if (null != os) {
    while (isRecording) {
        read = audioRecord.read(data, 0, recordBufSize);
　　　　  // 如果读取音频数据没有出现错误,就将数据写入到文件
        if (AudioRecord.ERROR_INVALID_OPERATION != read) {
            try {
                os.write(data);
            } catch (IOException e) {
                e.printStackTrace();
            }
        }
    }
    try {
        os.close();
    } catch (IOException e) {
        e.printStackTrace();
    }
}
2.5  关闭数据流
修改标志位：
isRecording为false,
上面的while循环就自动停止了,
数据流也就停止流动了,Stream也就被关闭了。
isRecording = false;
2.6  停止录音
停止录音之后,注意要释放资源。
if (null != audioRecord) {
	audioRecord.stop();
	audioRecord.release();
	audioRecord = null;
   	recordingThread = null;
}
权限需求：WRITE_EXTERNAL_STORAGE、RECORD_AUDIO
1)、
我按照流程,把音频数据都输出到文件里面了,
停止录音后,打开此文件,发现不能播放,到底是为什么呢？
答：
按照流程走完了,数据是进去了,但是现在的文件里面的内容
仅仅是最原始的音频数据,术语称为raw(中文解释是“原材料”或“未经处理的东西”),
这时候,你让播放器去打开,它既不知道保存的格式是什么,
又不知道如何进行解码操作。当然播放不了。
2)、
那如何才能在播放器中播放我录制的内容呢？
答： 
在文件的数据开头加入WAVE HEAD数据即可,也就是文件头。
只有加上文件头部的数据,播放器才能正确的知道里面的内容
到底是什么,进而能够正常的解析并播放里面的内容。
附言
Android SDK提供了两套音频采集的API,
分别是：MediaRecorder和AudioRecord,
前者是一个更加上层一点的API,
它可以直接把手机麦克风录入的音频数据
进行编码压缩(如AMR、MP3等)并存成文件,
而后者则更接近底层,能够更加自由灵活地控制,
可以得到原始的一帧帧PCM音频数据。
如果想简单地做一个录音机,录制成音频文件,
则推荐使用MediaRecorder,
而如果需要对音频做进一步的算法处理、
或者采用第三方的编码库进行压缩、
以及网络传输等应用,则建议使用AudioRecord,
其实MediaRecorder底层也是调用了
AudioRecord与Android Framework层的AudioFlinger进行交互的。
直播中实时采集音频自然是要用AudioRecord了。




Android音视频开发(三)：使用AudioTrack播放PCM音频
一、AudioTrack基本使用
AudioTrack类可以完成Android平台上音频数据的输出任务。
AudioTrack有两种数据加载模式(MODE_STREAM和MODE_STATIC),
对应的是数据加载模式和音频流类型,对应着两种完全不同的使用场景。
MODE_STREAM：
在这种模式下,通过write一次次把音频数据写到AudioTrack中。
这和平时通过write系统调用往文件中写数据类似,
但这种工作方式每次都需要把数据从用户提供的
Buffer中拷贝到AudioTrack内部的Buffer中,
这在一定程度上会使引入延时。
为解决这一问题,AudioTrack就引入了第二种模式。
MODE_STATIC：
这种模式下,在play之前只需要把所有数据通过一次write调用
传递到AudioTrack中的内部缓冲区,后续就不必再传递数据了。
这种模式适用于像铃声这种内存占用量较小,延时要求较高的文件。
但它也有一个缺点,就是一次write的数据不能太多,
否则系统无法分配足够的内存来存储全部数据。
1.1 MODE_STATIC模式
MODE_STATIC模式输出音频的方式如下
(注意：如果采用STATIC模式,须先调用write写数据,然后再调用play。)
1.2 MODE_STREAM模式
MODE_STREAM模式输出音频的方式如下:
int readCount = 0;
byte[] tempBuffer = new byte[bufferSize];
while (dis.available() > 0) {
    readCount = dis.read(tempBuffer);
    if (readCount == AudioTrack.ERROR_INVALID_OPERATION 
    	|| readCount == AudioTrack.ERROR_BAD_VALUE) {
        continue;
    }
    if (readCount != 0 && readCount != -1) {
        audioTrack.play();
        audioTrack.write(tempBuffer, 0, readCount);
    }
}
二、AudioTrack详解
2.1 音频流的类型
在AudioTrack构造函数中,
会接触到AudioManager.STREAM_MUSIC这个参数。
它的含义与Android系统对音频流的管理和分类有关。
Android将系统的声音分为好几种流类型,下面是几个常见的：
·  STREAM_ALARM：		警告声
·  STREAM_MUSIC：		音乐声,例如music等
·  STREAM_RING：			铃声
·  STREAM_SYSTEM：		系统声音,例如低电提示音,锁屏音等
·  STREAM_VOCIE_CALL：	通话声
注意：
上面这些类型的划分和音频数据本身并没有关系。
例如MUSIC和RING类型都可以是某首MP3歌曲。
另外,声音流类型的选择没有固定的标准,
例如,铃声预览中的铃声可以设置为MUSIC类型。
音频流类型的划分和Audio系统对音频的管理策略有关。
2.2 Buffer分配和Frame的概念
在计算Buffer分配的大小的时候,
我们经常用到的一个方法就是：getMinBufferSize。
这个函数决定了应用层分配多大的数据Buffer。
AudioTrack.getMinBufferSize(8000,// 每秒8K个采样点                              
　　      AudioFormat.CHANNEL_CONFIGURATION_STEREO,// 双声道                  
        AudioFormat.ENCODING_PCM_16BIT);
从AudioTrack.getMinBufferSize开始追溯代码,
可以发现在底层的代码中有一个很重要的概念：Frame(帧)。
Frame是一个单位,用来描述数据量的多少。
1单位的Frame等于1个采样点的字节数×声道数
(比如PCM16,双声道的1个Frame等于2×2=4字节)。
1个采样点只针对一个声道,而实际上可能会有一或多个声道。
由于不能用一个独立的单位来表示全部声道一次采样的数据量,
也就引出了Frame的概念。Frame的大小,就是一个采样点的字节数×声道数。
另外,在目前的声卡驱动程序中,其内部缓冲区也是
采用Frame作为单位来分配和管理的。
getMinBufSize会综合考虑硬件的情况
(诸如是否支持采样率,硬件本身的延迟情况等)后,
得出一个最小缓冲区的大小。一般我们分配的缓冲大小会是它的整数倍。
2.3 AudioTrack构造过程
每一个音频流对应着一个AudioTrack类的一个实例,
每个AudioTrack会在创建时注册到 AudioFlinger中,
由AudioFlinger把所有的AudioTrack进行混合(Mixer),
然后输送到AudioHardware中进行播放,
目前Android同时最多可以创建32个音频流,也就是说,
Mixer最多会同时处理32个AudioTrack的数据流。
三、 AudioTrack与MediaPlayer的对比
播放声音可以用MediaPlayer和AudioTrack,
两者都提供了Java API供应用开发者使用。
虽然都可以播放声音,但两者还是有很大的区别的。
3.1 区别
其中最大的区别是MediaPlayer可以播放多种格式的声音文件,
例如MP3,AAC,WAV,OGG,MIDI等。
MediaPlayer会在framework层创建对应的音频解码器。
而AudioTrack只能播放已经解码的PCM流,
如果对比支持的文件格式的话则是AudioTrack只支持wav格式的音频文件,
因为wav格式的音频文件大部分都是PCM流。
AudioTrack不创建解码器,所以只能播放不需要解码的wav文件。
3.2 联系
MediaPlayer在framework层还是会创建AudioTrack,
把解码后的PCM数据流传递给AudioTrack,
AudioTrack再传递给AudioFlinger进行混音,
然后才传递给硬件播放,所以是MediaPlayer包含了AudioTrack。
3.3 SoundPool
在接触Android音频播放API的时候,
发现SoundPool也可以用于播放音频。
下面是三者的使用场景：
MediaPlayer更加适合在后台长时间播放本地音乐文件或者在线的流式资源; 
SoundPool则适合播放比较短的音频片段,
比如游戏声音、按键声、铃声片段等等,它可以同时播放多个音频; 
而AudioTrack则更接近底层,提供了非常强大的控制能力,
支持低延迟播放,适合流媒体和VoIP语音电话等场景。




Android音视频开发(四)：使用Camera API采集视频数据
使用Camera API采集视频数据并保存到文件,
分别使用SurfaceView、TextureView来预览Camera数据,取到NV21的数据回调。
注：需要权限：<uses-permission android:name="android.permission.CAMERA" />
一、预览Camera数据
二、取到NV21的数据回调
Android中Google支持的Camera Preview Callback的YUV常用格式有两种：
一个是NV21,
一个是YV12。
Android一般默认使用YCbCr_420_SP的格式(NV21)。
可以配置数据回调的格式：
Camera.Parameters parameters = camera.getParameters();
parameters.setPreviewFormat(ImageFormat.NV21);
camera.setParameters(parameters);
// 打开摄像头并将展示方向旋转90度
camera = Camera.open();
camera.setDisplayOrientation(90);
通过setPreviewCallback方法监听预览的回调：
camera.setPreviewCallback(
	new Camera.PreviewCallback() {
    @Override
    public void onPreviewFrame(byte[] bytes, Camera camera) {
    	// 这里面的bytes的数据就是NV21格式的数据
    }
});




Android音视频开发(五)：使用MediaExtractor和MediaMuxer API解析和封装mp4文件
一个音视频文件是由音频和视频组成的,
我们可以通过MediaExtractor、MediaMuxer把音频或视频给单独抽取出来,
抽取出来的音频和视频能单独播放.
一、MediaExtractor API介绍
MediaExtractor的作用是把音频和视频的数据进行分离。
主要API介绍：
	// 即可以设置本地文件又可以设置网络文件
	setDataSource(String path)：
	// 得到源文件通道数 
	getTrackCount()：
	// 获取指定(index)的通道格式
	getTrackFormat(int index)：
	// 返回当前的时间戳 
	getSampleTime()：
	// 把指定通道中的数据按偏移量读取到ByteBuffer中；
	readSampleData(ByteBuffer byteBuf, int offset)：
	// 读取下一帧数据
	advance()：
	// 读取结束后释放资源
	release(): 
示例:
MediaExtractor extractor = new MediaExtractor();
extractor.setDataSource(...);
int numTracks = extractor.getTrackCount();
for (int i = 0; i < numTracks; ++i) {
	MediaFormat format = extractor.getTrackFormat(i);
	String mime = format.getString(MediaFormat.KEY_MIME);
	if (weAreInterestedInThisTrack) {
		extractor.selectTrack(i);
	}
}
ByteBuffer inputBuffer = ByteBuffer.allocate(...)
while (extractor.readSampleData(inputBuffer, ...) >= 0) {
	int trackIndex = extractor.getSampleTrackIndex();
	long presentationTimeUs = extractor.getSampleTime();
	...
	extractor.advance();
}
extractor.release();
extractor = null;
二、MediaMuxer API介绍
MediaMuxer的作用是生成音频或视频文件；
还可以把音频与视频混合成一个音视频文件。
相关API介绍：
	// path:输出文件的名称  format:输出文件的格式；当前只支持MP4格式；
	MediaMuxer(String path, int format)
	// 添加通道；我们更多的是使用
	// MediaCodec.getOutpurForma()
	// Extractor.getTrackFormat(int index)
	// 来获取MediaFormat;也可以自己创建；
	addTrack(MediaFormat format)
	// 开始合成文件
	start()
	// 把ByteBuffer中的数据写入到在构造器设置的文件中；
	writeSampleData(int trackIndex, ByteBuffer byteBuf, 
					MediaCodec.BufferInfo bufferInfo)
	// 停止合成文件
	stop()
	// 释放资源
	release()
示例:
MediaMuxer muxer = new MediaMuxer("temp.mp4", OutputFormat.MUXER_OUTPUT_MPEG_4);
// More often, the MediaFormat will be retrieved from MediaCodec.getOutputFormat()
// or MediaExtractor.getTrackFormat().
MediaFormat audioFormat = new MediaFormat(...);
MediaFormat videoFormat = new MediaFormat(...);
int audioTrackIndex = muxer.addTrack(audioFormat);
int videoTrackIndex = muxer.addTrack(videoFormat);
ByteBuffer inputBuffer = ByteBuffer.allocate(bufferSize);
boolean finished = false;
BufferInfo bufferInfo = new BufferInfo();
muxer.start();
while(!finished) {
	// getInputBuffer() will fill the inputBuffer with one frame of encoded
	// sample from either MediaCodec or MediaExtractor, set isAudioSample to
	// true when the sample is audio data, set up all the fields of bufferInfo,
	// and return true if there are no more samples.
	finished = getInputBuffer(inputBuffer, isAudioSample, bufferInfo);
	if (!finished) {
		int currentTrackIndex = isAudioSample ? audioTrackIndex : videoTrackIndex;
		muxer.writeSampleData(currentTrackIndex, inputBuffer, bufferInfo);
	}
}
muxer.stop();
muxer.release();




Android音视频开发(六)： MediaCodec API详解
下面开始接触一个Android音视频中相当重要的一个API： MediaCodec。
通过这个API,我们能够做很多Android音视频方面的工作,
下面是我们学习这个API的时候,主要的方向：
学习 MediaCodec API,完成音频 AAC   硬编、硬解
学习 MediaCodec API,完成视频 H.264 硬编、硬解
一、MediaCodec介绍
MediaCodec类可以用于使用一些基本的多媒体编解码器(音视频编解码组件),
它是Android基本的多媒体支持基础架构的一部分.
通常和MediaExtractor, MediaSync, MediaMuxer, 
MediaCrypto, MediaDrm, Image, Surface, AudioTrack一起使用。
一个编解码器可以处理输入的数据来产生输出的数据,
编解码器使用一组输入和输出缓冲器来异步处理数据。
你可以创建一个空的输入缓冲区,填充数据后发送到编解码器进行处理。
编解码器使用输入的数据进行转换,然后输出到一个空的输出缓冲区。
最后你获取到输出缓冲区的数据,消耗掉里面的数据,释放回编解码器。
如果后续还有数据需要继续处理,编解码器就会重复这些操作。

ImageFormat:
图像压缩格式
	RGB_565
	YV12(YUV420P)
	Y8
	Y16
	NV16
	NV21(YUV420SP)
	YUY2
	JPEG
	YUV_420_888
	YUV_422_888
	YUV_444_888
	FLEX_RGB_888
	FLEX_RGBA_8888
	RAW_SENSOR
	RAW_PRIVATE
	RAW10
	RAW12
	DEPTH16
	DEPTH_POINT_CLOUD
	PRIVATE

Audio Application Framework：音频应用框架 
	AudioTrack：负责回放数据的输出,属 Android 应用框架 API 类
	AudioRecord：负责录音数据的采集,属 Android 应用框架 API 类
	AudioSystem： 负责音频事务的综合管理,属 Android 应用框架 API 类

Audio Native Framework：音频本地框架 
	AudioTrack：负责回放数据的输出,属 Android 本地框架 API 类
	AudioRecord：负责录音数据的采集,属 Android 本地框架 API 类
	AudioSystem： 负责音频事务的综合管理,属 Android 本地框架 API 类

Audio Services：音频服务 
	AudioPolicyService：音频策略的制定者,负责音频设备切换的策略抉择、音量调节策略等
	AudioFlinger：音频策略的执行者,负责输入输出流设备的管理及音频流数据的处理传输

Audio HAL：音频硬件抽象层,负责与音频硬件设备的交互,由 AudioFlinger 直接调用

与Audio相关的有MultiMedia,MultiMedia负责音视频的编解码,
MultiMedia将解码后的数据通过AudioTrack输出,
而AudioRecord采集的录音数据交由MultiMedia进行编码。

AudioTrack Java API 两种数据传输模式：
	MODE_STATIC	
		应用进程将回放数据一次性付给 AudioTrack,
		适用于数据量小、时延要求高的场景
	MODE_STREAM	
		用进程需要持续调用write()写数据到FIFO,
		写数据时有可能遭遇阻塞(等待 AudioFlinger::PlaybackThread 消费之前的数据),
		基本适用所有的音频场景
AudioTrack Java API 音频流类型：
	STREAM_VOICE_CALL		电话语音
	STREAM_SYSTEM			系统声音
	STREAM_RING				铃声声音,如来电铃声、闹钟铃声等
	STREAM_MUSIC			音乐声音
	STREAM_ALARM			警告音
	STREAM_NOTIFICATION		通知音
	STREAM_DTMF				DTMF音(拨号盘按键音)
应用开发者应该根据应用场景选择相应的流类型,以便系统为这道流选择合适的输出设备。
frameworks/base/media/tests/audiotests

采样深度以字节为单位
最小数据缓冲区的大小 = 最低帧数 * 声道数 * 采样深度
AudioTrack.getMinBufferSize();
最小数据缓冲区的大小,它是声音能正常播放的最低保障,
从函数参数来看,返回值取决于采样率、采样深度、声道数这三个属性。
MODE_STREAM 模式下,应用程序重点参考其返回值然后确定分配多大的数据缓冲区。
如果数据缓冲区分配得过小,那么播放声音会频繁遭遇underrun,
underrun 是指生产者(AudioTrack)提供数据的速度跟不上
消费者(AudioFlinger::PlaybackThread)消耗数据的速度,
反映到现实的后果就是声音断续卡顿,严重影响听觉体验。

最低帧数
AudioTrack::getMinFrameCount()
首先要了解音频领域中,帧(frame)的概念：
帧表示一个完整的声音单元,所谓的声音单元是指一个采样样本；
如果是双声道,那么一个完整的声音单元就是 2 个样本;
如果是 5.1 声道,那么一个完整的声音单元就是 6 个样本了。
帧的大小(一个完整的声音单元的数据量)等于声道数乘以采样深度,
即 frameSize = channelCount * bytesPerSample。
帧的概念非常重要,无论是框架层还是内核层,都是以帧为单位去管理音频数据缓冲区的。
其次还得了解音频领域中,传输延迟(latency)的概念：
传输延迟表示一个周期的音频数据的传输时间。
可能有些读者一脸懵逼,一个周期的音频数据,这又是啥？
我们再引入周期(period)的概念：
Linux ALSA 把数据缓冲区划分为若干个块,
dma 每传输完一个块上的数据即发出一个硬件中断,
cpu 收到中断信号后,再配置 dma 去传输下一个块上的数据；
一个块即是一个周期,周期大小(periodSize)即是一个数据块的帧数。
再回到传输延迟(latency),传输延迟等于周期大小除以采样率,
即 latency = periodSize / sampleRate。
最后了解下音频重采样：
音频重采样是指这样的一个过程——把一个采样率的数据转换为另一个采样率的数据。
Android 原生系统上，音频硬件设备一般都工作在一个固定的采样率上（如 48 KHz），
因此所有音轨数据都需要重采样到这个固定的采样率上，然后再输出。
为什么这么做？系统中可能存在多个音轨同时播放，
而每个音轨的采样率可能是不一致的；比如在播放音乐的过程中，
来了一个提示音，这时需要把音乐和提示音混音并输出到硬件设备，
而音乐的采样率和提示音的采样率不一致，问题来了，
如果硬件设备工作的采样率设置为音乐的采样率的话，那么提示音就会失真；
因此最简单见效的解决方法是：硬件设备工作的采样率固定一个值，
所有音轨在 AudioFlinger 都重采样到这个采样率上，
混音后输出到硬件设备，保证所有音轨听起来都不失真。

声音采集
麦克风采集到声音后转化为模拟电信号，
之后需要将模拟电信号数字化转化为计算机能够识别的模拟信号。
Android中利用AudioRecord可以录制声音，
录制出来的声音可以设置为PCM声音。
想要将声音用计算机语言表述，则必须将声音进行数字化。
将声音数字化，最常见的方式是通过
脉冲编码调制PCM(Pulse Code Modulation) 。
声音经过麦克风，转换成一连串电压变化的信号。
要将这样的电压变化的信号转化成为PCM信号则需要进行三个过程：
抽样、量化、编码。
要实现这三个过程，则需要使用三个参数，
它们是：采样频率、采样位数和声道数。
1、采样频率
采样频率即取样频率，指每秒钟取得声音样本的次数。采样频率越高，声音的质量也就越好，声音的还原也就越真实，但同时它占的资源比较多。由于人耳的分辨率很有限，太高的频率并不能分辨出来。在16位声卡中有22KHz、44KHz等几级，其中，22KHz相当于普通FM广播的音质，44KHz已相当于CD音质了，目前的常用采样频率都不超过48KHz。
2、采样位数
采样位数即采样值或取样值（就是将采样样本幅度量化）。它是用来衡量声音波动变化的一个参数，也可以说是声卡的分辨率。它的数值越大，分辨率也就越高，所发出声音的能力越强。
在计算机中采样位数一般有8位和16位之分，8位不是说把纵坐标分成8份，而是分成2的8次方即256份； 同理16位是把纵坐标分成2的16次方65536份。
采样率和采样大小的值越大，记录的波形更接近原始信号。
3、声道数
很好理解，有单声道和立体声之分，单声道的声音只能使用一个喇叭发声（有的也处理成两个喇叭输出同一个声道的声音），立体声的pcm可以使两个喇叭都发声（一般左右声道有分工） ，更能感受到空间效果。
那么，现在我们就可以得到pcm文件所占容量的公式：
存储量 = (采样频率 · 采样位数 · 声道 · 时间)／8 (单位：字节数)

声音录制
<uses-permission android:name="android.permission.RECORD_AUDIO"/>
Android中使用AudioRecord录制声音，
需要传递给AudioRecord采样频率、采样位数和声道数，
除此之外还需要传入两个参数，一个是声音源，一个是缓冲区大小。
音频源
下面是Android支持的音频源：
/** 默认声音 **/
public static final int DEFAULT = 0;
/** 麦克风声音 */
public static final int MIC = 1;
/** 通话上行声音 */
public static final int VOICE_UPLINK = 2;
/** 通话下行声音 */
public static final int VOICE_DOWNLINK = 3;
/** 通话上下行声音 */
public static final int VOICE_CALL = 4;
/** 根据摄像头转向选择麦克风*/
public static final int CAMCORDER = 5;
/** 对麦克风声音进行声音识别，然后进行录制 */
public static final int VOICE_RECOGNITION = 6;
/** 对麦克风中类似ip通话的交流声音进行识别，默认会开启回声消除和自动增益 */
public static final int VOICE_COMMUNICATION = 7;
/** 录制系统内置声音 */
public static final int REMOTE_SUBMIX = 8;
缓冲区大小
麦克风采集到的数据先放置在一个缓冲区里面，
之后我们再从这个缓冲区里面读取数据，
从而获取到麦克风录制的音频数据。
在Android中不同的声道数、采样位数和采样频率会有不同的最小缓冲区大小，
当AudioRecord传入的缓冲区大小小于最小缓冲区大小的时候则会初始化失败。
大的缓冲区大小可以达到更为平滑的录制效果，相应的也会带来更大一点的延时。

在对AudioRecord进行录音前需要对采样率进行设置，
对于采样率，Android官方文档要求所有的手机需要对44100Hz的采样率进行支持.
以下是几种常见的采样率：
8000, 11025, 16000, 22050, 44100, 48000
在设置采样率之前需要对手机对设置的采样率是否支持进行检测，
下面是一段代码是获取手机支持的音频采样率：
public void getValidSampleRates() {
    for (int rate : new int[] {8000, 11025, 16000, 22050, 44100}) {  // add the rates you wish to check against
        int bufferSize = AudioRecord.getMinBufferSize(rate, AudioFormat.CHANNEL_CONFIGURATION_DEFAULT, AudioFormat.ENCODING_PCM_16BIT);
        if (bufferSize > 0) {
            // buffer size is valid, Sample rate supported

        }
    }
}
为了达到立体声的效果，Android也是支持多个声道采集的，
一些情况下为了完成在所有手机上的视频，
我们需要把声道设置为AudioFormat.CHANNEL_CONFIGURATION_MONO（单声道）。
对于采样位数，选的值也是以常量的形式定义在 AudioFormat 类中，
常用的是 
ENCODING_PCM_16BIT（16bit）
ENCODING_PCM_8BIT （8bit）
ENCODING_PCM_16BIT可以保证兼容所有Android手机的。
AudioRecord有一个状态量用来表示AudioRecord是否被成功初始化，
通过getState()方法可以获取，
当返回为STATE_UNINITIALIZED表示未成功初始化，
当返回为STATE_INITIALIZED表示已经成功初始化了。

参数选择
Android手机有很多厂商，对于开发者来说兼容性一直以来都是一个重要的问题。
在录音过程中，Android推荐的参数如下：
sampleRateInHz = 44100;
channelConfig = AudioFormat.CHANNEL_CONFIGURATION_MONO;
audioFormat = AudioFormat.ENCODING_PCM_16BIT

回声消除
在Android中回声消除可以通过三种方式进行处理：
1、
通过VOICE_COMMUNICATION模式进行录音，自动实现回声消除；
2、
利用Android自身带的AcousticEchoCanceler进行回声消除处理；
3、
使用第三方库（Speex、Webrtc）进行回声消除处理。
关于第1种方式,
使用AudioRecord模式进行录音的时候，
需要将AudioManager设置模式为MODE_IN_COMMUNICATION，
还需要将麦克风打开。
有一点需要特别注意，音频采样率必须设置8000或者16000，通道数必须设为1个。
AudioManager audioManager = (AudioManager)mContext.getSystemService(Context.AUDIO_SERVICE);
audioManager.setMode(AudioManager.MODE_IN_COMMUNICATION);
audioManager.setSpeakerphoneOn(true);

声音模式
在Android系统中有着多种的声音模式，
通过AudioManager.setMode()可以设置声音的模式。
就像上面回声消除所描述的，通过设置声音模式为MODE_IN_COMMUNICATION，
加上一些声音参数的设置可以启动Android自身的硬件回声消除（通话时候的回声消除）。
设置声音模式的时候需要权限“android.permission.MODIFY_AUDIO_SETTINGS”。
不同的声音模式声音的输出行为不一样。
当设置为MODE_IN_COMMUNICATION模式时，声音默认是听筒出声，
这时候如果是在连麦模式而且主播没有戴耳机的情况下显然这样不符合，
这时候需要调用audioManager.setSpeakerphoneOn(true)切换成外放出声。
当插上耳机后，声音不需要外放，需要从耳机出声，
这样可以设置audioManager.setSpeakerphoneOn(false)。
当声音模式为MODE_NORMAL，没有插耳机的时候声音自动外放，
插上耳机声音从耳机出声，不需要进行相应的设置。



