Android音视频的编解码

android_atomic_inc()原子操作函数
android_atomic_add()
const_cast<RefBase*>
Android中提出了一套类似Java垃圾回收机制的智能指针，采用强指针sp（Strong Pointer）和弱指针wp（Weak Pointer）对目标对象进行应用，实现对象的自动回收。
对象可以分为全局对象、局部对象、静态全局对象和静态局部对象。
Android设计了强引用sp和弱引用wp，故实际对象的释放，可分为强引用控制和弱引用控制。所谓强引用控制，指的是强引用数mStrong为0时，释放实际对象；弱引用控制，则指的是弱引用数mWeak为0时，才释放实际对象。


// encoder
result = avcodec_send_frame(audioAVCodecContext, frame);
result = avcodec_receive_packet(audioAVCodecContext, avPacket);

// decoder
result = avcodec_send_packet(audioAVCodecContext, avPacket);
result = avcodec_receive_frame(audioAVCodecContext, decoded_frame);


cd x264
CC=cl ./configure --enable-static --enable-shared --enable-pic


 ./configure --toolchain=msvc --enable-yasm --enable-asm --enable-gpl --enable-libx264 --extra-cflags=-I/usr/local/include --extra-ldflags=-LIBPATH:/usr/local/lib



 ./configure --prefix=/root/mydev/tools/ffmpeg --enable-libmp3lame --enable-static --enable-shared --enable-x86asm --enable-asm --enable-gpl --enable-libx264


Stream #0.0[0x1e0]: Video: mpeg2video, yuv420p, 704x576 [PAR 12:11 DAR 4:3], 9578 kb/s, 25 tbr, 90k tbn, 50 tbc
25  tbr 代表帧率
90k tbn 代表文件层的时间精度,即1S=1200k,和duration相关
50  tbc 代表视频层的时间精度,即1S=50,和strem->duration和时间戳相关

在AndroidAPI <= 20（Android5.0之前的版本）中Google支持的CameraPreview Callback的YUV常用格式有两种：一个是NV21，一个是YV12。如果我们需要对Camera采集的图像进行编码等，必须要对其进一步处理，比如格式转换、旋转等操作，否则会出现一些花屏、叠影等问题。	


采样率：每秒钟记录多少个采样点；
在H264协议里定义了三种帧，完整编码的帧叫I帧，参考之前的I帧生成的只包含差异部分编码的帧叫P帧，还有一种参考前后的帧编码的帧叫B帧。
H264采用的核心算法是帧内压缩和帧间压缩，帧内压缩是生成I帧的算法，帧间压缩是生成B帧和P帧的算法。

ffmpeg提供了av_rescale_q_rnd函数进行转换。
av_rescale_q_rnd(int64_t a, int64_t b, int64_t c, enum AVRounding rnd)
此函数主要用于对于不同时间戳的转换。具体来说是将原来以 "时间基b" 表示的 数值a 转换成以 "时间基c" 来表示的新值。AVRounding表示取整的策略

ffmpeg -i /media/1.WAV -acodec libmp3lame /media/1.MP3
ffmpeg -i apple.mp4 -f mp3 -vn apple.mp3
参数解释：
-i 表示input，即输入文件
-f 表示format，即输出格式
-vn表示vedio not，即输出不包含视频
对比源视频文件和提取得到的音频文件大小，可以看到源视频文件为约23M，而提取出来的音频文件大小为3M。

FFmpeg还提供了很多有用的工具可以查看和处理音视频文件，如：
查看视频文件的音视频编解码格式，视频时长，比特率等，如下：
dennis@ubuntu:~$ ffmpeg -i apple.mp4

ffmpeg -i test.mp4 -vcodec libx264 -b:v 1200k -r 25 -acodec mp3 -ab 128k -ar 44100 output.mp4

ffmpeg -codecs
ffmpeg -codecs | grep aac

[whb@jcwkyl introduction_to_algorithm]$ ffmpeg -i Lecture_1.flv -f mp2 -vn Lecture_1.mp3
这条命令中，-i表示input file，-f表示输出格式，-vn表示“vedio not"，即禁止视频输出，最后转换后的文件是Lecture_1.mp3。
转换完成后，使用file命令查看Lecture_1.mp3的文件格式：
[whb@jcwkyl introduction_to_algorithm]$ file Lecture_1.mp3
Lecture_1.mp3: MPEG ADTS, layer II, v2,  64 kBits, 22.05 kHz, Stereo
转换前后文件大小对比：
[whb@jcwkyl introduction_to_algorithm]$ du -hs Lecture_1.*
153M    Lecture_1.flv
37M     Lecture_1.mp3
使用播放器播放Lecture_1.mp3，完全正常。



avCodec = avcodec_find_encoder_by_name("libfdk_aac");

常用的立体声有2个通道，环绕立体声3个通道。
声道数目 立体声（stero）单声道（mono）

采样率  单位时间内对音频ad芯片的采样次数，常见的音频采样率(sample_rate)有。
 0: 96000 Hz
 1: 88200 Hz
 2: 64000 Hz
 3: 48000 Hz
 4: 44100 Hz
 5: 32000 Hz
 6: 24000 Hz
 7: 22050 Hz
 8: 16000 Hz
 9: 12000 Hz
10: 11025 Hz
11: 8000 Hz
12: 7350 Hz
13: Reserved
14: Reserved
15: frequency is written explicitly
左边是adts头里面对应的采样率编码值，右边是对应的采样率值。
单采样bit数目  单个采样的bit位数，16／8 bit。

音频
32 kbps —MW(AM) 质量
96 kbps —FM质量
128 - 160 kbps –相当好的质量，有时有明显差别
192 kbps — 优良质量，偶尔有差别
224 - 320 kbps — 高质量
音频
800 bps – 能够分辨的语音所需最低码率（需使用专用的FS-1015语音编解码器）
8 kbps —电话质量（使用语音编码）
8-500 kbps --Ogg Vorbis和MPEG1 Player1/2/3中使用的有损音频模式
500 kbps–1.4 Mbps —44.1KHz的无损音频，解码器为FLAC Audio,WavPack或Monkey's Audio
1411.2 - 2822.4 Kbps —脉冲编码调制(PCM)声音格式CD光碟的数字音频
5644.8 kbps —SACD使用的Direct Stream Digital格式

ffmpeg中的一些参数：
   sample_format    音频采样格式
   sample_rate      采样率值（非编码值）
   channel_layout   解码后的PCM数据layout格式，左左左右右右   左右左右左右
   nb_samples       一帧音频中的采样个数，用于计算一帧数据大小

注意ffmpeg中的两个结构，AVPacket(编码的音视频帧),AVFrame(解码后的音视频数据)
AVPacket packet = {0};
packet.data =(uint8_t*)buf;
    packet.size = len;
AVFrame *decode_frame = avcodec_alloc_frame();
if(decode_frame == NULL)
{
return -1;
}
//前面已经初始化过解码器
int audio4_decode_len = avcodec_decode_audio4(av_codec_ctx_,decode_frame,got_frame,&packet);//解码
if(audio4_decode_len < 0||*got_frame != 1)
{
return -1;
}
AVSampleFormat out_sample_format = AV_SAMPLE_FMT_S16;
struct SwrContext *audio_convert_ctx;
        audio_convert_ctx = swr_alloc();
unsigned long long in_channel_layout = av_get_default_channel_layout(av_codec_ctx_->channels);
                    audio_convert_ctx = swr_alloc_set_opts(audio_convert_ctx,in_channel_layout,out_sample_format,av_codec_ctx_->sample_rate,
                                    in_channel_layout,av_codec_ctx_->sample_fmt,av_codec_ctx_->sample_rate,0,NULL);
swr_init(audio_convert_ctx);                   
    int nb_samples = swr_convert(audio_convert_ctx,//重采样，返回的值是新的采样率的一帧中的采样个数，可以根据声道和采样bit数目计算一帧数据大小
                       (uint8_t **)&outbuf,
                       AVCODEC_MAX_AUDIO_FRAME_SIZE,
                       (const uint8_t **)decode_frame->data,
                       decode_frame->nb_samples);
outlen = av_samples_get_buffer_size(NULL,av_codec_ctx_->channels ,nb_samples,out_sample_format, 1);//计算出重采样后一帧pcm数据的大小
sample_rate = av_codec_ctx_->sample_rate;
channels = av_codec_ctx_->channels;
avcodec_free_frame(&decode_frame);

封装格式
作用：视频码流和音频码流按照一定的格式储存在一个文件汇总
视频编码数据
作用：将视频像素数据（RGB，YUV等）压缩成为视频码流，从而降低视频的数据量
音频编码数据
作用：将音频采样数据（PCM等）压缩成为音频码流，从而降低音频的数据量
视频像素数据
作用：保存了屏幕上每一个像素点的像素值
格式：常见的像素数据格式有RGB24, RGB32, YUV420P,YUV422P,YUV444P等。压缩编码中一般使用的是YUV格式的像素数据,最为常见的格式为YUV420P。
特点：视频像素数据体积很大，一般情况下一小时高清视频的RGB24格式的数据体积为：3600*25*1920*1080*3=559.9GB（PS：这里假定帧率为25HZ，取样精度8bit）
音频采样数据
作用：保存了音频中每个采样点的值。
特点：音频采样数据体积很大,一般情况下一首4分钟的PCM格式的
   歌曲体积为:
   4*60*44100*2*2=42.3MByte
          PS:这里假定采样率为44100Hz,采样精度为16bit


ffmpeg解码
OpenGL绘图
OpenGL-PBO加速
OpenGL-Double PBO加速
YUV格式转RGB(由GPU完成)
多线程解码
多路视频
视频特效


音频格式的plane概念（平面）
enum AVSampleFormat {
    AV_SAMPLE_FMT_NONE = -1,
    AV_SAMPLE_FMT_U8,          ///< unsigned 8 bits
    AV_SAMPLE_FMT_S16,         ///< signed 16 bits
    AV_SAMPLE_FMT_S32,         ///< signed 32 bits
    AV_SAMPLE_FMT_FLT,         ///< float
    AV_SAMPLE_FMT_DBL,         ///< double

	// 以下都是带平面格式(带P的都是分片的)
    AV_SAMPLE_FMT_U8P,         ///< unsigned 8 bits, planar
    AV_SAMPLE_FMT_S16P,        ///< signed 16 bits, planar
    AV_SAMPLE_FMT_S32P,        ///< signed 32 bits, planar
    AV_SAMPLE_FMT_FLTP,        ///< float, planar
    AV_SAMPLE_FMT_DBLP,        ///< double, planar

    AV_SAMPLE_FMT_NB           ///< Number of sample formats. DO NOT USE if linking dynamically
};
同样对双声道音频PCM数据，以S16P为例，存储就可能是
plane 0: LLLLLLLLLLLLLLLLLLLLLLLLLL...
plane 1: RRRRRRRRRRRRRRRRRRRRRRRRRR...
而不再是以前的连续buffer。

mp3编码明确规定了只使用平面格式的数据
AAC编码依旧使用AV_SAMPLE_FMT_S16格式
输入也可能是分平面的
存储PCM数据，注意：swr_context即使进行了转换，也要判断转换后的数据是否分平面
av_sample_fmt_is_planar(pCodecCtx->sample_fmt)
编码格式要求是分平面数据
对于解码也可能需要做swr_convert，比如做播放器，很多时候我们是将S16格式数据丢给声卡，而新版ffmpeg解码音频输出的格式可能不满足S16，如AAC解码后得到的是FLT（浮点型），AC3解码是FLTP（带平面）等，需要根据具体的情况决定是否需要convert

假设现在有2个通道channel1, channel2.
那么AV_SAMPLE_FMT_S16在内存的格式就为: c1, c2, c1, c2, c1, c2, ....
而AV_SAMPLE_FMT_S16P在内存的格式为: c1, c1, c1,... c2, c2, c2,...

关于音频分片的问题
	1：无论是不是分片的数据总量是相同的.
	2：分片的存储在内存中linesize如果两声道则左右分开占用linesize[0]和linesize[1].
	3：不是分片的存储在内存中两声道不分开，左右左右....这样存储，只占用linesize[0].
音频信息
	switch (id) {
	case AV_CODEC_ID_ADPCM_ADX:    return   32;
	case AV_CODEC_ID_ADPCM_IMA_QT: return   64;
	case AV_CODEC_ID_ADPCM_EA_XAS: return  128;
	case AV_CODEC_ID_AMR_NB:
	case AV_CODEC_ID_EVRC:
	case AV_CODEC_ID_GSM:
	case AV_CODEC_ID_QCELP:
	case AV_CODEC_ID_RA_288:       return  160;
	case AV_CODEC_ID_AMR_WB:
	case AV_CODEC_ID_GSM_MS:       return  320;
	case AV_CODEC_ID_MP1:          return  384;
	case AV_CODEC_ID_ATRAC1:       return  512;
	case AV_CODEC_ID_ATRAC3:       return 1024 * framecount;
	case AV_CODEC_ID_ATRAC3P:      return 2048;
	case AV_CODEC_ID_MP2:
	case AV_CODEC_ID_MUSEPACK7:    return 1152;
	case AV_CODEC_ID_AC3:          return 1536;
	}
	AAC格式nb_samples和frame_size是1024

	如果音频，样本：s16;采样率：44100；声道：2。
	av_get_bytes_per_sample(s16) == 2;
	1：假设从麦克风或者文件读出来的通过av_read_frame得到一个数据总量是88200个字节。
	   这个88200个字节是和帧无关的数据量。
	2:如果接下来需要将这些数据编码成(无论要编码成AAC还MP3都需要用到ffmpeg的fifo或者AVAudioFifo做数据缓冲)：
		1) AAC:
		nb_samples和frame_size = 1024
		一帧数据量：1024*2*av_get_bytes_per_sample(s16) = 4096个字节。
		编码：88200/(1024*2*av_get_bytes_per_sample(s16)) = 21.5帧数据
		2) MP3:
		nb_samples和frame_size = 1152
		一帧数据量：1152*2*av_get_bytes_per_sample(s16) = 4608个字节。
		编码：88200/(1152*2*av_get_bytes_per_sample(s16)) = 19.1帧数据
	3:持续时间方面
		1) AAC
		音频帧的播放时间=一个AAC帧对应的采样样本的个数/采样频率(单位为s)
		一帧 1024个 sample。采样率 Samplerate 44100KHz，每秒44100个sample, 所以根据公式   音频帧的播放时间=一个AAC帧对应的采样样本的个数/采样频率
		当前AAC一帧的播放时间是= 1024*1000000/44100= 22.2ms(单位为ms)
		2) MP3
		mp3 每帧均为1152个字节， 则：
		frame_duration = 1152 * 1000000 / sample_rate
		例如：sample_rate = 44100HZ时，计算出的时长为26.122ms，这就是经常听到的mp3每帧播放时间固定为26ms的由来

ffmpeg重采样中swr_convert和swr_get_out_samples的用法
在做mux的时候关于重采样可以用fifo，或者audiofifo做缓存处理，当做demux的时候关于重采样就可以用到上面的swr_convert和swr_get_out_samples做配合处理。
就说如果传入的nb_samles大于了传出的nb_samplse则SwrContext中会有缓存，会导致内存一直暴涨，解决方法，可以看如下代码：
	没有缓存的重采样这么处理：
	    ret = swr_convert(swrcontext, pOutputFrame->data,pOutputFrame->nb_samples,
	    					(const uint8_t**)pInputFrame->data,pInputFrame->nb_samples);

	有缓存的代码这么处理：
	    //如果还有缓存在swrcontext中,第二个参数要填写0才能获取到，缓存数据
	    int fifo_size = swr_get_out_samples(swrcontext,0);
	    if ( fifo_size >= pOutputFrame->nb_samples)
	    {
	    	ret = swr_convert(swrcontext, pOutputFrame->data,pOutputFrame->nb_samples,
	    						NULL,0);
	    }
即如果有缓存则先判断是否有缓存在里面，如果有则传入数据为空取出缓存。


音频重采样
ffmpeg实现音频重采样的核心函数swr_convert功能非常强大，可是ffmpeg文档对它的注释太过简单，在应用中往往会出这样那样的问题，其实在读取数据->重采样->编码数据的循环中在第一次执行swr_convert后还应用swr_convert再作个缓存检测看看是否还有数据，如果有就要把它写到FIFO中去，留在下次再使用，这点在转码和由低向高转换采样率时特别重要。
	const int frame_size = FFMIN(fifo_size, m_Opt->encode_pCodecCtx->frame_size);
 
 
	if ((ret = av_audio_fifo_read(m_fifo, (void **)m_fifo_samples_array, frame_size)) < frame_size) {
		fprintf(stderr, "Could not read data from FIFO\n");
		return AVERROR_EXIT;
	}
 
	int out_samples = av_rescale_rnd(swr_get_delay(m_Opt->out_resample_context, 48000) + 1536, 44100, 48000, AV_ROUND_UP);
 
 
	int conver_samples= swr_convert(m_Opt->out_resample_context, m_fifo_conver_samples_array, frame_size,
		(const uint8_t **)m_fifo_samples_array, frame_size);
	ret = av_audio_fifo_size(m_conver_fifo);
 
	if ((ret = av_audio_fifo_realloc(m_conver_fifo, av_audio_fifo_size(m_conver_fifo) + frame_size)) < 0) {
			fprintf(stderr, "Could not reallocate FIFO\n");
			return ret;
		}
 
	ret = av_audio_fifo_write(m_conver_fifo, (void **)m_fifo_conver_samples_array, conver_samples);
		fifo_size = av_audio_fifo_size(m_conver_fifo);
 
		if ((ret = av_audio_fifo_read(m_conver_fifo, (void **)m_Opt->out_samples_array, frame_size)) < frame_size) {
			fprintf(stderr, "Could not read data from FIFO\n");
			return AVERROR_EXIT;
		}
		
		ret = Encode_audio(ret);
 
		int ret1 = 0;
 
		while ((ret1 = swr_convert(m_Opt->out_resample_context, m_fifo_conver_samples_array, frame_size, NULL, 0)) > 0)
	{
	
 
		if ((ret = av_audio_fifo_realloc(m_conver_fifo, av_audio_fifo_size(m_conver_fifo) + ret1)) < 0) {
			fprintf(stderr, "Could not reallocate FIFO\n");
			return ret;
		}
 
		ret = av_audio_fifo_write(m_conver_fifo, (void **)m_fifo_conver_samples_array, ret1);
		fifo_size = av_audio_fifo_size(m_conver_fifo);
 
		if (fifo_size > m_Opt->encode_pFrame->nb_samples)
		{
 
			if ((ret = av_audio_fifo_read(m_conver_fifo, (void **)m_Opt->out_samples_array, frame_size)) < frame_size) {
				fprintf(stderr, "Could not read data from FIFO\n");
				return AVERROR_EXIT;
			}
 
			ret = Encode_audio(ret);
 
		}

重点在：
    if ((r = swr_convert(swr_ctx, output, output_nb_samples,(const uint8_t**)input, nb_samples)) < 0)
        return -1;
 
    while ((r = swr_convert(swr_ctx, output, output_nb_samples, NULL, 0)) > 0) {
 
    }


重采样和AVAudioFifo的用法
    SwrContext * ffmpeg_init_pcm_resample(Out_stream_info * out_stream_info,AVFrame *in_frame, AVFrame *out_frame)
    {
    	SwrContext * swr_ctx = NULL;  
    	swr_ctx = swr_alloc();  
    	if (!swr_ctx)  
    	{  
    		printf("swr_alloc error \n");  
    		return NULL;  
    	}  
    	AVCodecContext * audio_dec_ctx = m_icodec->streams[m_in_audio_stream_idx]->codec;  
    	AVSampleFormat sample_fmt;  
    	sample_fmt = (AVSampleFormat)out_stream_info->m_dwBitsPerSample; //样本  
    	int out_channel_layout = av_get_default_channel_layout(out_stream_info->m_dwChannelCount);
    	if (audio_dec_ctx->channel_layout == 0)  
    	{  
    		audio_dec_ctx->channel_layout = av_get_default_channel_layout(m_icodec->streams[m_in_audio_stream_idx]->codec->channels);  
    	}  
    	/* set options */  
    	av_opt_set_int(swr_ctx, "in_channel_layout",    audio_dec_ctx->channel_layout, 0);  
    	av_opt_set_int(swr_ctx, "in_sample_rate",       audio_dec_ctx->sample_rate, 0);  
    	av_opt_set_sample_fmt(swr_ctx, "in_sample_fmt", audio_dec_ctx->sample_fmt, 0);  
    	av_opt_set_int(swr_ctx, "out_channel_layout",   out_channel_layout, 0);   
    	av_opt_set_int(swr_ctx, "out_sample_rate",       out_stream_info->m_dwFrequency, 0);  
    	av_opt_set_sample_fmt(swr_ctx, "out_sample_fmt", sample_fmt, 0);  
    	swr_init(swr_ctx);  
     
    	int64_t src_nb_samples = in_frame->nb_samples; 
    	//计算输出的samples 和采样率有关 例如：48000转44100，samples则是从1152转为1059，除法
    	out_frame->nb_samples = av_rescale_rnd(src_nb_samples, out_stream_info->m_dwFrequency, audio_dec_ctx->sample_rate, AV_ROUND_UP);
     
    	int ret = av_samples_alloc(out_frame->data, &out_frame->linesize[0],   
    		out_stream_info->m_dwChannelCount, out_frame->nb_samples,out_stream_info->m_oaudio_st->codec->sample_fmt,1);  
    	if (ret < 0)  
    	{  
    		return NULL;  
    	}  
     
    	out_stream_info->m_audiofifo  = av_audio_fifo_alloc(out_stream_info->m_oaudio_st->codec->sample_fmt, out_stream_info->m_oaudio_st->codec->channels,  
    		out_frame->nb_samples);   
     
    	return swr_ctx;  
    }
     
    int ffmpeg_preform_pcm_resample(Out_stream_info * out_stream_info,SwrContext * pSwrCtx,AVFrame *in_frame, AVFrame *out_frame)
    {
    	int ret = 0;
    	int samples_out_per_size = 0;              //转换之后的samples大小
      
    	if (pSwrCtx != NULL)   
    	{  
    		//这里注意下samples_out_per_size这个值和 out_frame->nb_samples这个值有时候不一样，ffmpeg里面做了策略不是问题。
    		samples_out_per_size = swr_convert(pSwrCtx, out_frame->data, out_frame->nb_samples,   
    			(const uint8_t**)in_frame->data, in_frame->nb_samples);  
    		if (samples_out_per_size < 0)  
    		{  
    			return -1;  
    		}  
     
    		AVCodecContext * audio_dec_ctx = m_icodec->streams[m_in_audio_stream_idx]->codec; 
     
    		int buffersize_in = av_samples_get_buffer_size(&in_frame->linesize[0],audio_dec_ctx->channels,  
    			in_frame->nb_samples, audio_dec_ctx->sample_fmt, 1);
     
    		//修改分包内存  
    		int buffersize_out = av_samples_get_buffer_size(&out_frame->linesize[0], out_stream_info->m_oaudio_st->codec->channels,  
    			samples_out_per_size, out_stream_info->m_oaudio_st->codec->sample_fmt, 1); 
     
    		int fifo_size = av_audio_fifo_size(out_stream_info->m_audiofifo);  
    		fifo_size = av_audio_fifo_realloc(out_stream_info->m_audiofifo, av_audio_fifo_size(out_stream_info->m_audiofifo) + out_frame->nb_samples);  
    		av_audio_fifo_write(out_stream_info->m_audiofifo,(void **)out_frame->data,samples_out_per_size);  
    		fifo_size = av_audio_fifo_size(out_stream_info->m_audiofifo); 
     
    		out_frame->pkt_pts = in_frame->pkt_pts;  
    		out_frame->pkt_dts = in_frame->pkt_dts;  
    		//有时pkt_pts和pkt_dts不同，并且pkt_pts是编码前的dts,这里要给avframe传入pkt_dts而不能用pkt_pts  
    		//out_frame->pts = out_frame->pkt_pts;  
    		out_frame->pts = in_frame->pkt_dts;  
     
    		//测试用
    		if (out_stream_info->user_stream_id ==11)
    		{
    			if (pcm_file == NULL)
    			{
    				pcm_file = fopen("11.pcm","wb");
    			}
    			int wtiresize = fwrite(out_frame->data[0],buffersize_out,1, pcm_file);
    			fflush(pcm_file);
    		}
    	}  
    	ret = 1;
    	return ret;
    }
     
    void ffmpeg_uinit_pcm_resample(SwrContext * swr_ctx,AVAudioFifo * audiofifo)
    { 
    	if (swr_ctx)  
    	{  
    		swr_free(&swr_ctx);  
    		swr_ctx = NULL;  
    	}  
    	if(audiofifo)  
    	{  
    		av_audio_fifo_free(audiofifo);  
    		audiofifo = NULL;  
    	}     
    }

FFmpeg关于nb_smples,frame_size以及profile的解释
原来一直记得固定编码格式需要固定的sample,例如下面：
1) AAC:
nb_samples和frame_size = 1024
一帧数据量：1024*2*av_get_bytes_per_sample(s16) = 4096个字节。
会编码：88200/(1024*2*av_get_bytes_per_sample(s16)) = 21.5帧数据
2) MP3:
nb_samples和frame_size = 1152
一帧数据量：1152*2*av_get_bytes_per_sample(s16) = 4608个字节。
MP3:则会编码：88200/(1152*2*av_get_bytes_per_sample(s16)) = 19.1帧数据

但最近发现AAC编码的音频nb_sampes和frame_size,nb_samplse是avframe中的，frame_szie是AVCodecContext中的，有可能出现2048的情况，一直以为是样本是分片planner例如AV_SAMPLE_FMT_FLT，AV_SAMPLE_FMT_FLTP这些导致，但后来发现无关。aac编码中感谢网友摘录了一些注释，如下：
    /*
    A HE-AAC v1 or v2 audio frame contains 2048 PCM samples per channel (there is
    also one mode with 1920 samples per channel but this is only for special purposes
    such as DAB+ digital radio).
    These bits/frame figures are average figures where each AAC frame generally has a different
    size in bytes. To calculate the same for AAC-LC just use 1024 instead of 2048 PCM samples per
    frame and channel.
    For AAC-LD/ELD it is either 480 or 512 PCM samples per frame and channel.
    */
从中会发现 当aac编码级别是LC时frame_size 和nb_samples是1024，如果是HE的时候是2048。

//这里的最后一个参数一定要注意用pInputFrame->nb_samples* per_sample_in，以AAC举例子，AVCodecContext中的profile会有LC，HE等不同，
//nb_samples在LC的时候是1024，在HE的时候是2048。如果不填写对会影响音频数据,nb_samples和AVCodecContext中的frame_size相同。
ret = avcodec_fill_audio_frame(pInputFrame,Channel_in,SampleFormat_in,buf_in,buf_size_in,pInputFrame->nb_samples* per_sample_in); 

其种标记在ffmpeg中是AVCodecContext中的profile：
    /**
     * profile
     * - encoding: Set by user.
     * - decoding: Set by libavcodec.
     */
     int profile;
其值如下:
#define FF_PROFILE_UNKNOWN -99
#define FF_PROFILE_RESERVED -100
 
#define FF_PROFILE_AAC_MAIN 0
#define FF_PROFILE_AAC_LOW  1
#define FF_PROFILE_AAC_SSR  2
#define FF_PROFILE_AAC_LTP  3
#define FF_PROFILE_AAC_HE   4
#define FF_PROFILE_AAC_HE_V2 28
#define FF_PROFILE_AAC_LD   22
#define FF_PROFILE_AAC_ELD  38
#define FF_PROFILE_MPEG2_AAC_LOW 128
#define FF_PROFILE_MPEG2_AAC_HE  131
 
#define FF_PROFILE_DTS         20
#define FF_PROFILE_DTS_ES      30
#define FF_PROFILE_DTS_96_24   40
#define FF_PROFILE_DTS_HD_HRA  50
#define FF_PROFILE_DTS_HD_MA   60
 
#define FF_PROFILE_MPEG2_422    0
#define FF_PROFILE_MPEG2_HIGH   1
#define FF_PROFILE_MPEG2_SS     2
#define FF_PROFILE_MPEG2_SNR_SCALABLE  3
#define FF_PROFILE_MPEG2_MAIN   4
#define FF_PROFILE_MPEG2_SIMPLE 5
 
#define FF_PROFILE_H264_CONSTRAINED  (1<<9)  // 8+1; constraint_set1_flag
#define FF_PROFILE_H264_INTRA        (1<<11) // 8+3; constraint_set3_flag
 
#define FF_PROFILE_H264_BASELINE             66
#define FF_PROFILE_H264_CONSTRAINED_BASELINE (66|FF_PROFILE_H264_CONSTRAINED)
#define FF_PROFILE_H264_MAIN                 77
#define FF_PROFILE_H264_EXTENDED             88
#define FF_PROFILE_H264_HIGH                 100
#define FF_PROFILE_H264_HIGH_10              110
#define FF_PROFILE_H264_HIGH_10_INTRA        (110|FF_PROFILE_H264_INTRA)
#define FF_PROFILE_H264_HIGH_422             122
#define FF_PROFILE_H264_HIGH_422_INTRA       (122|FF_PROFILE_H264_INTRA)
#define FF_PROFILE_H264_HIGH_444             144
#define FF_PROFILE_H264_HIGH_444_PREDICTIVE  244
#define FF_PROFILE_H264_HIGH_444_INTRA       (244|FF_PROFILE_H264_INTRA)
#define FF_PROFILE_H264_CAVLC_444            44
 
#define FF_PROFILE_VC1_SIMPLE   0
#define FF_PROFILE_VC1_MAIN     1
#define FF_PROFILE_VC1_COMPLEX  2
#define FF_PROFILE_VC1_ADVANCED 3
 
#define FF_PROFILE_MPEG4_SIMPLE                     0
#define FF_PROFILE_MPEG4_SIMPLE_SCALABLE            1
#define FF_PROFILE_MPEG4_CORE                       2
#define FF_PROFILE_MPEG4_MAIN                       3
#define FF_PROFILE_MPEG4_N_BIT                      4
#define FF_PROFILE_MPEG4_SCALABLE_TEXTURE           5
#define FF_PROFILE_MPEG4_SIMPLE_FACE_ANIMATION      6
#define FF_PROFILE_MPEG4_BASIC_ANIMATED_TEXTURE     7
#define FF_PROFILE_MPEG4_HYBRID                     8
#define FF_PROFILE_MPEG4_ADVANCED_REAL_TIME         9
#define FF_PROFILE_MPEG4_CORE_SCALABLE             10
#define FF_PROFILE_MPEG4_ADVANCED_CODING           11
#define FF_PROFILE_MPEG4_ADVANCED_CORE             12
#define FF_PROFILE_MPEG4_ADVANCED_SCALABLE_TEXTURE 13
#define FF_PROFILE_MPEG4_SIMPLE_STUDIO             14
#define FF_PROFILE_MPEG4_ADVANCED_SIMPLE           15
 
#define FF_PROFILE_JPEG2000_CSTREAM_RESTRICTION_0   0
#define FF_PROFILE_JPEG2000_CSTREAM_RESTRICTION_1   1
#define FF_PROFILE_JPEG2000_CSTREAM_NO_RESTRICTION  2
#define FF_PROFILE_JPEG2000_DCINEMA_2K              3
#define FF_PROFILE_JPEG2000_DCINEMA_4K              4 

自此，frame_size,nb_samples，profile之间的关系可以理清楚了，关于mp3的只发现过1152的，如果有其他再记录。 


pcm裸流播放方法
ubuntu 工作环境，
使用命令：sudo apt-get install sox下载播放工具。
完成后，可以使用play命令播放pcm裸流文件了。
首先需要知道对应pcm的文件的采样率，通道数，比特率，
然后使用命令
play -t raw -r 44.1k -e signed-integer -b 16 -c 2 loved.pcm
loved.pcm  这个是文件名
-t 参数后接文件包含数据的格式，我还没高明白raw和pcm这2个音频缩写词包含的具体含义。
-r 数据采样率参数 44.1k
-b 数据比特率参数 8--->128kbps 16--->256kbps 32--->512kbps
-c 通道数        2
下面这个参数没明白，初步猜测是一个数据类型的设置。
-e -e|--encoding ENCODING   Set encoding (ENCODING may be one of 
						 signed-integer(-b 16 or -b 32)一般指定这个值,
                         unsigned-integer, 
                         floating-point, 
                         mu-law, 
                         a-law,
                         ima-adpcm, 
                         ms-adpcm, 
                         gsm-full-rate)

一个多媒体文件包含有多个原始流，例如 movie.mkv这个多媒体文件可能包含下面的流数据
原始流 1 h.264 video
原始流 2 aac audio for Chinese
原始流 3 aac audio for English
原始流 4 Chinese Subtitle
原始流 5 English Subtitle

音频基本概念
1.
通道数
声音在录制时在不同空间位置用不同录音设备采样的声音信号，声音在播放时采用相应个数的扬声器播放。采用多通道的方式是为了丰富声音的现场感。常用的立体声有2个通道，环绕立体声3个通道。数字音频就是有一连串的样本流组成，立体声每次采用要采两次。有点类似视频中的YUV各个分量。
2.
采样率
把模拟信号转换成数字信号在计算机中处理，需要按照一定的采样率采样，样本值就是声音波形中的一个值。音频在播放时按照采样率进行，采样率越高声音的连续性就越好，由于人的听觉器官分辨能力的局限，往往这些数值达到某种程度就可以满足人对“连续”性的需求了。例如22050和44100的采样率就是电台和CD 常用的采样率。类似视频中的帧率。
3.
比特率(bps或kbps)
单位时间所需的空间存储。比特率反应的是视频或者音频一个样本所有的信息量，越大含有的信息量就大。视频中，图像分辨率越大，一帧就越大，实时解码就容易饥渴，传输带宽需要越大，存储空间就越大。音频中描述一个样本就越准确。
4.
帧
视频中帧就是一个图片采样。音频中一帧一般包含多个样本，如AAC格式会包含1024个样本。
5.
样本格式
音频中一个样本存储方式。列举ffmpeg中的样本格式
enum AVSampleFormat {
    AV_SAMPLE_FMT_NONE = -1,
    AV_SAMPLE_FMT_U8,          ///< unsigned 8 bits
    AV_SAMPLE_FMT_S16,         ///< signed 16 bits
    AV_SAMPLE_FMT_S32,         ///< signed 32 bits
    AV_SAMPLE_FMT_FLT,         ///< float
    AV_SAMPLE_FMT_DBL,         ///< double
 
    AV_SAMPLE_FMT_U8P,         ///< unsigned 8 bits, planar
    AV_SAMPLE_FMT_S16P,        ///< signed 16 bits, planar
    AV_SAMPLE_FMT_S32P,        ///< signed 32 bits, planar
    AV_SAMPLE_FMT_FLTP,        ///< float, planar
    AV_SAMPLE_FMT_DBLP,        ///< double, planar
 
    AV_SAMPLE_FMT_NB           ///< Number of sample formats. DO NOT USE if linking dynamically
};
讲一下AV_SAMPLE_FMT_S16和AV_SAMPLE_FMT_S16P格式，AV_SAMPLE_FMT_S16保存一个样本采用有符号16bit交叉存储的方式，AV_SAMPLE_FMT_S16P保存一个样本采用有符号16bit平面存储的方式。举例有两个通道，通道1数据流 c1 c1 c1c1... , 通道2数据流 c2 c2 c2 c2...
平面存储方式：c1 c1 c1c1... c2 c2 c2 c2...
交叉存储方式：c1, c2,c1, c2, c1, c2, ...
AVFrame中平面方式planar每个通道数据存储在data[0], data[1]中，
长度为linesize[0],linesize[1],
交叉方式则所有的数据都存储在data[0],长度为linesize[0]。

mplayer -demuxer rawvideo -rawvideo w=500:h=500 haoke.yuv

ffmpeg采集摄像头视频数据
ffmpeg -f video4linux2 -s 320*240 -r 10 -i /dev/video0 test.asf

ffmpeg2.0最新的解码出来的数据是 sample_fmts  = AV_SAMPLE_FMT_FLTP
android需要的音频格式：sample_fmts (AV_SAMPLE_FMT_S8, AV_SAMPLE_FMT_S16) 

问题:




通过av_get_channel_layout_nb_channels()和av_get_default_channel_layout()这些函数可以得到channels和channellayout的转换。
libavutil中的audioconvert.c定义channellayout和channels的相关map：
channel_layout_map[]
	{ "mono",        1,  AV_CH_LAYOUT_MONO },
    { "stereo",      2,  AV_CH_LAYOUT_STEREO },
    { "2.1",         3,  AV_CH_LAYOUT_2POINT1 },
    { "3.0",         3,  AV_CH_LAYOUT_SURROUND },
    { "3.0(back)",   3,  AV_CH_LAYOUT_2_1 },
    { "4.0",         4,  AV_CH_LAYOUT_4POINT0 },
    { "quad",        4,  AV_CH_LAYOUT_QUAD },
    { "quad(side)",  4,  AV_CH_LAYOUT_2_2 },
    { "3.1",         4,  AV_CH_LAYOUT_3POINT1 },
    { "5.0",         5,  AV_CH_LAYOUT_5POINT0_BACK },
    { "5.0(side)",   5,  AV_CH_LAYOUT_5POINT0 },
    { "4.1",         5,  AV_CH_LAYOUT_4POINT1 },
    { "5.1",         6,  AV_CH_LAYOUT_5POINT1_BACK },
    { "5.1(side)",   6,  AV_CH_LAYOUT_5POINT1 },
    { "6.0",         6,  AV_CH_LAYOUT_6POINT0 },
    { "6.0(front)",  6,  AV_CH_LAYOUT_6POINT0_FRONT },
    { "hexagonal",   6,  AV_CH_LAYOUT_HEXAGONAL },
    { "6.1",         7,  AV_CH_LAYOUT_6POINT1 },
    { "6.1",         7,  AV_CH_LAYOUT_6POINT1_BACK },
    { "6.1(front)",  7,  AV_CH_LAYOUT_6POINT1_FRONT },
    { "7.0",         7,  AV_CH_LAYOUT_7POINT0 },
    { "7.0(front)",  7,  AV_CH_LAYOUT_7POINT0_FRONT },
    { "7.1",         8,  AV_CH_LAYOUT_7POINT1 },
    { "7.1(wide)",   8,  AV_CH_LAYOUT_7POINT1_WIDE },
    { "octagonal",   8,  AV_CH_LAYOUT_OCTAGONAL },
    { "downmix",     2,  AV_CH_LAYOUT_STEREO_DOWNMIX, },


PCM音频数据是如何存储的
单声道音频,采样数据按时间的先后顺序依次存入
双声道音频,按照LRLRLR的方式存储,存储的时候还和机器的大小端有关

8位单声道:0声道,0声道......
8位双声道:0声道(左),1声道(右),0声道(左),1声道(右)......
16位单声道:0声道(低字节),0声道(高字节),0声道(低字节),0声道(高字节)......
16位双声道:0声道(左,低字节),0声道(左,高字节),1声道(右,低字节),1声道(右,高字节)......

PCM音频数据中常用的专业术语
一般我们描述PCM音频数据的参数的时候有如下描述方式:
44100Hz 16bit stereo:每秒钟有44100次采样,采样数据用16位(2字节)记录,双声道(立体声)；
22050Hz 8bit  mono:每秒钟有22050次采样,采样数据用8位(1字节)记录,单声道；
 44100Hz指的是采样率，它的意思是每秒取样44100次。采样率越大，存储数字音频所占的空间就越大。
16bit指的是采样精度，意思是原始模拟信号被采样后，每一个采样点在计算机中用16位（两个字节）来表示。采样精度越高越能精细地表示模拟信号的差异。
一般来说PCM数据中的波形幅值越大，代表音量越大。










































